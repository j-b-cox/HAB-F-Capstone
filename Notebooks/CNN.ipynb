{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e21aefe1-4bb6-4f61-9557-b68d57542952",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path    = \"../LabelData/combined_wavelengths.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9384b6f9-d835-4f14-ae03-23b5f58dcb52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# --- 1. Load your raw dataset ---\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Assume `raw_dataset` is a list of (img_array, species_idx) pairs,\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# where img_array has shape (10,10,19) and species_idx is an int 0..C-1.\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# For example, you might have loaded it like:\u001b[39;00m\n\u001b[32m     14\u001b[39m raw = np.load(dataset_path, allow_pickle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m raw_dataset = \u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Here we just reference `raw_dataset`.\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# ----------------------------------------------------------------\u001b[39;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# --- 2. Group by identical images into sets of species labels ---\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhash_image\u001b[39m(arr: np.ndarray) -> \u001b[38;5;28mbytes\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# --- 1. Load your raw dataset ---\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Assume `raw_dataset` is a list of (img_array, species_idx) pairs,\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# where img_array has shape (10,10,19) and species_idx is an int 0..C-1.\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# For example, you might have loaded it like:\u001b[39;00m\n\u001b[32m     14\u001b[39m raw = np.load(dataset_path, allow_pickle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m raw_dataset = [(\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m, item[\u001b[32m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m raw]\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Here we just reference `raw_dataset`.\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# ----------------------------------------------------------------\u001b[39;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# --- 2. Group by identical images into sets of species labels ---\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhash_image\u001b[39m(arr: np.ndarray) -> \u001b[38;5;28mbytes\u001b[39m:\n",
      "\u001b[31mIndexError\u001b[39m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# --- 1. Load your raw dataset ---\n",
    "# Assume `raw_dataset` is a list of (img_array, species_idx) pairs,\n",
    "# where img_array has shape (10,10,19) and species_idx is an int 0..C-1.\n",
    "# For example, you might have loaded it like:\n",
    "raw = np.load(dataset_path, allow_pickle=True)\n",
    "raw_dataset = [(item[1], item[0]) for item in raw]\n",
    "#\n",
    "# Here we just reference `raw_dataset`.\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# --- 2. Group by identical images into sets of species labels ---\n",
    "def hash_image(arr: np.ndarray) -> bytes:\n",
    "    \"\"\"Produce a hashable key for a numpy array of floats.\"\"\"\n",
    "    # We use tostring of bytes; if memory is an issue you can md5 it instead.\n",
    "    return arr.tobytes()\n",
    "import hashlib\n",
    "\n",
    "def hash_image(arr: np.ndarray, decimals: int = 6) -> str:\n",
    "    \"\"\"\n",
    "    Round the array to `decimals` places, then hash via MD5.\n",
    "    This groups “bit-noisy” floats that are equal up to rounding,\n",
    "    and gives a compact string key.\n",
    "    \"\"\"\n",
    "    # 1) Round to reduce spurious bit-noise\n",
    "    rounded = np.round(arr, decimals=decimals)\n",
    "    # 2) Convert to bytes\n",
    "    b = rounded.tobytes()\n",
    "    # 3) MD5 digest (hex string)\n",
    "    return hashlib.md5(b).hexdigest()\n",
    "\n",
    "# Rebuild your “bags” mapping\n",
    "from collections import defaultdict\n",
    "\n",
    "bags = defaultdict(set)\n",
    "image_lookup = {}\n",
    "\n",
    "for img, species in raw_dataset:\n",
    "    key = hash_image(img, decimals=6)\n",
    "    bags[key].add(species)\n",
    "    if key not in image_lookup:\n",
    "        image_lookup[key] = img.copy()   # store one representative\n",
    "\n",
    "# Now reconstruct unique images + multi‐species sets\n",
    "images    = []\n",
    "label_sets = []\n",
    "for key, species_set in bags.items():\n",
    "    images.append(image_lookup[key])\n",
    "    label_sets.append(species_set)\n",
    "images = np.stack(images, axis=0)  # shape (N_unique, 10, 10, 19)\n",
    "\n",
    "# Reconstruct unique arrays + label sets\n",
    "images = []\n",
    "label_sets = []\n",
    "for key, species_set in bags.items():\n",
    "    images.append(image_lookup[key])\n",
    "    label_sets.append(species_set)\n",
    "images = np.stack(images, axis=0)        # shape (N,10,10,19)\n",
    "images_raw = images.copy()\n",
    "\n",
    "# --- 3. Multi-hot encode labels ---\n",
    "all_species = sorted({s for sset in label_sets for s in sset})\n",
    "mlb = MultiLabelBinarizer(classes=all_species)\n",
    "Y = mlb.fit_transform(label_sets)        # shape (N, C), dtype int\n",
    "\n",
    "# --- 4. Filter out fully blank images (all channels NaN) ---\n",
    "# Blank if every pixel in every band is NaN:\n",
    "valid = ~np.all(np.isnan(images).reshape(images.shape[0], -1), axis=1)\n",
    "images = images[valid]\n",
    "Y      = Y[valid]\n",
    "\n",
    "# --- 4b. (Optional) Drop images with > X% blank pixels ---\n",
    "# First, rebuild a per‐image “blankness” mask over the original 19 bands\n",
    "# Note: `images_raw` should be your (N,10,10,19) array *before* you zero‐filled NaNs  \n",
    "# (so you might need to keep that around).\n",
    "\n",
    "# Suppose you still have `images_raw` from step 4, before you do mask/zero‐fill:\n",
    "# images_raw = images.copy()  \n",
    "\n",
    "# Compute a boolean mask: True where *all* bands are NaN\n",
    "blank_pixel_mask = np.all(np.isnan(images_raw), axis=-1)   # shape (N,10,10)\n",
    "\n",
    "# Compute fraction of blank pixels per image\n",
    "blank_frac = blank_pixel_mask.reshape(images_raw.shape[0], -1).mean(axis=1)  \n",
    "# blank_frac[i] is between 0.0 (no blank pixels) and 1.0 (all pixels blank)\n",
    "\n",
    "# Choose your threshold:\n",
    "X = 0.95     # e.g. drop any image with ≥30% completely‐blank pixels\n",
    "\n",
    "# Keep only the “good” images\n",
    "keep = blank_frac < X\n",
    "print(f\"Dropping {np.count_nonzero(~keep)} / {len(keep)} images (≥{X*100:.0f}% blank)\")\n",
    "\n",
    "# Filter both your features and labels:\n",
    "images_raw = images_raw[keep]\n",
    "label_sets  = [ls for ls, k in zip(label_sets, keep) if k]\n",
    "\n",
    "# --- 5. Build a mask channel & replace NaNs with zero ---\n",
    "# mask = 1 where any band is non-NaN; 0 where all NaN\n",
    "mask = np.any(~np.isnan(images), axis=-1, keepdims=True).astype('float32')\n",
    "# zero-fill the 19 bands\n",
    "images = np.nan_to_num(images, nan=0.0).astype('float32')\n",
    "# concatenate mask as 20th channel\n",
    "images = np.concatenate([images, mask], axis=-1)  # (N,10,10,20)\n",
    "\n",
    "# --- 5b. Per-channel normalization of the 19 data bands ---\n",
    "# Compute mean/std only over the original 19 channels\n",
    "means = np.nanmean(images[..., :19], axis=(0,1,2))\n",
    "stds  = np.nanstd( images[..., :19], axis=(0,1,2))\n",
    "# Broadcast and normalize\n",
    "images[..., :19] = (images[..., :19] - means) / (stds + 1e-6)\n",
    "\n",
    "\n",
    "# --- 6. Train/Val split ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    images, Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- 7. Build the MIL CNN model ---\n",
    "num_classes = Y.shape[1]  # C\n",
    "inp = Input(shape=(10,10,20))  # 19 bands + 1 mask\n",
    "\n",
    "# Instance encoder\n",
    "x = layers.Conv2D(32, 3, padding='same', activation='relu')(inp)\n",
    "x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "# Flatten spatial dims into instance dimension\n",
    "instances = layers.Reshape((10*10, 64))(x)          # (batch,100,64)\n",
    "# Per-instance embedding\n",
    "inst_embed = layers.Dense(128, activation='relu')(instances)  # (batch,100,128)\n",
    "# MIL pooling (max over the 100 instances)\n",
    "bag_embed = layers.GlobalMaxPooling1D()(inst_embed)          # (batch,128)\n",
    "# Multi-label output\n",
    "out = layers.Dense(num_classes, activation='sigmoid')(bag_embed)\n",
    "\n",
    "model = Model(inputs=inp, outputs=out)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# --- 8. Callbacks for training ---\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "# --- 9. Train ---\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# --- 10. Final evaluation ---\n",
    "val_loss, val_auc = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(f\"Final val loss: {val_loss:.4f}, val AUC: {val_auc:.4f}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 1. Plot training history\n",
    "def plot_history(history):\n",
    "    # Loss\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(history.history['loss'], label='train loss')\n",
    "    plt.plot(history.history['val_loss'], label='val loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Overall AUC (if you logged it)\n",
    "    if 'auc' in history.history:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(history.history['auc'], label='train AUC')\n",
    "        plt.plot(history.history['val_auc'], label='val AUC')\n",
    "        plt.title('Overall AUC')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('AUC')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Call this after training:\n",
    "plot_history(history)\n",
    "\n",
    "\n",
    "# 2. Compute per-class AUC on the validation set\n",
    "# ----------------------------------------------------------------------------\n",
    "# You need:\n",
    "#   - X_val, y_val  : your validation features and multi-hot labels\n",
    "#   - model         : your trained Keras model\n",
    "#   - class_names   : a list/array of your class labels, e.g.\n",
    "#                     mlb.classes_ if you used sklearn's MultiLabelBinarizer\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred = model.predict(X_val)    # shape (n_val, C)\n",
    "y_true = y_val                   # shape (n_val, C)\n",
    "class_names = mlb.classes_       # or your own list of length C\n",
    "\n",
    "# Compute and print per‐class ROC-AUC\n",
    "print(\"Per‐class ROC AUC on validation set:\")\n",
    "for i, cls in enumerate(class_names):\n",
    "    # roc_auc_score requires at least one positive & one negative example per class\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true[:, i], y_pred[:, i])\n",
    "        print(f\"  Class {cls:<15} : AUC = {auc:.3f}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"  Class {cls:<15} : could not compute AUC ({e})\")\n",
    "        \n",
    "# --- 2b. Compute per‐class accuracy on the validation set ---\n",
    "# y_pred: numpy array of shape (n_val, C), floats in [0,1]\n",
    "# y_true: numpy array of shape (n_val, C), ints in {0,1}\n",
    "\n",
    "# 1) Binarize predictions at 0.5\n",
    "y_pred_label = (y_pred >= 0.5).astype(int)\n",
    "\n",
    "# 2) Compute accuracy for each class (over all samples)\n",
    "#    accuracy_i = (# correct on class i) / (total samples)\n",
    "per_class_acc = (y_pred_label == y_true).sum(axis=0) / y_true.shape[0]\n",
    "\n",
    "print(\"\\nPer‐class Accuracy on validation set:\")\n",
    "for cls, acc in zip(class_names, per_class_acc):\n",
    "    print(f\"  Class {cls:<15} : Accuracy = {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87cc17e2-d4d0-4400-bea9-c3ddbbf0c476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subset Accuracy (exact match): 0.407\n"
     ]
    }
   ],
   "source": [
    "subset_acc = np.all(y_pred_label == y_true, axis=1).mean()\n",
    "print(f\"\\nSubset Accuracy (exact match): {subset_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "142e365f-16ee-46b2-8376-be2e9645605a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample-wise Accuracy (Jaccard): 0.422\n"
     ]
    }
   ],
   "source": [
    "intersection = np.logical_and(y_true, y_pred_label).sum(axis=1)\n",
    "union = np.logical_or(y_true, y_pred_label).sum(axis=1)\n",
    "samplewise_acc = (intersection / np.clip(union, a_min=1, a_max=None)).mean()\n",
    "print(f\"Sample-wise Accuracy (Jaccard): {samplewise_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf96b90e-a28d-4f6a-9ea9-d7ae5140cf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class 0               : Precision = 0.700, Recall = 0.671, F1 = 0.685\n",
      "  Class 1               : Precision = 0.852, Recall = 0.479, F1 = 0.613\n",
      "  Class 2               : Precision = 1.000, Recall = 0.038, F1 = 0.074\n",
      "  Class 3               : Precision = 0.000, Recall = 0.000, F1 = 0.000\n",
      "  Class 4               : Precision = 0.000, Recall = 0.000, F1 = 0.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "for i, cls in enumerate(class_names):\n",
    "    p = precision_score(y_true[:, i], y_pred_label[:, i], zero_division=0)\n",
    "    r = recall_score(y_true[:, i], y_pred_label[:, i], zero_division=0)\n",
    "    f1 = f1_score(y_true[:, i], y_pred_label[:, i], zero_division=0)\n",
    "    print(f\"  Class {cls:<15} : Precision = {p:.3f}, Recall = {r:.3f}, F1 = {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25a59dde-0d11-48bd-bcaa-fdcc77c962d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Macro-Averaged Scores:\n",
      "  Precision = 0.510, Recall = 0.238, F1 = 0.275\n",
      "\n",
      "Micro-Averaged Scores:\n",
      "  Precision = 0.745, Recall = 0.397, F1 = 0.518\n"
     ]
    }
   ],
   "source": [
    "# Macro-average: unweighted mean over classes\n",
    "macro_precision = precision_score(y_true, y_pred_label, average='macro', zero_division=0)\n",
    "macro_recall    = recall_score(y_true, y_pred_label, average='macro', zero_division=0)\n",
    "macro_f1        = f1_score(y_true, y_pred_label, average='macro', zero_division=0)\n",
    "\n",
    "# Micro-average: global TP, FP, FN summed across all labels\n",
    "micro_precision = precision_score(y_true, y_pred_label, average='micro', zero_division=0)\n",
    "micro_recall    = recall_score(y_true, y_pred_label, average='micro', zero_division=0)\n",
    "micro_f1        = f1_score(y_true, y_pred_label, average='micro', zero_division=0)\n",
    "\n",
    "print(f\"\\nMacro-Averaged Scores:\")\n",
    "print(f\"  Precision = {macro_precision:.3f}, Recall = {macro_recall:.3f}, F1 = {macro_f1:.3f}\")\n",
    "\n",
    "print(f\"\\nMicro-Averaged Scores:\")\n",
    "print(f\"  Precision = {micro_precision:.3f}, Recall = {micro_recall:.3f}, F1 = {micro_f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a46c8a21-5e11-4596-8c47-77b3ead2691b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([73, 48, 26, 20, 17])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_val, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54cc71b-7c8e-44a4-8889-57396cca5f36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
