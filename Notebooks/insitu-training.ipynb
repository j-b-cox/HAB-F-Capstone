{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fed6190-246b-4189-84d5-1f511477678b",
   "metadata": {
    "id": "3fed6190-246b-4189-84d5-1f511477678b"
   },
   "source": [
    "## 1. Setup\n",
    "\n",
    "Begin by importing all of the packages used in this notebook. If your kernel uses an environment defined following the guidance on the [tutorials] page, then the imports will be successful.\n",
    "\n",
    "[tutorials]: https://oceancolor.gsfc.nasa.gov/resources/docs/tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b832fcd3",
   "metadata": {
    "executionInfo": {
     "elapsed": 662,
     "status": "ok",
     "timestamp": 1749570726761,
     "user": {
      "displayName": "Muthumayan Madhayyan",
      "userId": "14757534479618487320"
     },
     "user_tz": 420
    },
    "id": "b832fcd3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e85024de",
   "metadata": {
    "executionInfo": {
     "elapsed": 686,
     "status": "ok",
     "timestamp": 1749570753274,
     "user": {
      "displayName": "Muthumayan Madhayyan",
      "userId": "14757534479618487320"
     },
     "user_tz": 420
    },
    "id": "e85024de"
   },
   "outputs": [],
   "source": [
    "dataset = np.load('../LabelData/dataset.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psz2n3EUHW5F",
   "metadata": {
    "id": "psz2n3EUHW5F"
   },
   "source": [
    "## Mean and Standard deviation\n",
    "Compute the mean and stddev of pixels for each of those 19 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c0302f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1749570753281,
     "user": {
      "displayName": "Muthumayan Madhayyan",
      "userId": "14757534479618487320"
     },
     "user_tz": 420
    },
    "id": "67c0302f",
    "outputId": "3d08732f-9350-43e9-fed7-af7eee09123f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape: (940, 10, 10, 19)\n",
      "pixels.shape: (94000, 19)\n",
      "\n",
      "Channel Means (ignoring NaNs): [0.00018529 0.00197223 0.00315226 0.00513789 0.00603405 0.00401628\n",
      " 0.00471744 0.00849927 0.009139   0.00896765 0.00713521 0.00414049\n",
      " 0.0049448  0.00285523 0.00410934 0.00269009 0.00399925 0.00281253\n",
      " 0.00211597]\n",
      "Channel STDs (ignoring NaNs): [0.00519959 0.00396854 0.00399149 0.00340319 0.00370621 0.00541371\n",
      " 0.00554312 0.00466338 0.00491752 0.00480148 0.00595262 0.00486634\n",
      " 0.0040856  0.00444033 0.00357955 0.00435695 0.00340758 0.00437636\n",
      " 0.00397049]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract all image arrays\n",
    "images = np.array([item[1] for item in dataset])  # shape: (n, 10, 10, 19)\n",
    "print(\"images.shape:\", images.shape)\n",
    "\n",
    "# Step 2: Reshape to (n * 10 * 10, 19)\n",
    "pixels = images.reshape(-1, 19)\n",
    "print(\"pixels.shape:\", pixels.shape)\n",
    "print()\n",
    "\n",
    "# Step 3: Compute mean and std across all valid (non-NaN) pixels for each channel\n",
    "channel_means = np.nanmean(pixels, axis=0)\n",
    "channel_stds = np.nanstd(pixels, axis=0)\n",
    "\n",
    "print(\"Channel Means (ignoring NaNs):\", channel_means)\n",
    "print(\"Channel STDs (ignoring NaNs):\", channel_stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zfwgzT6BHlt-",
   "metadata": {
    "id": "zfwgzT6BHlt-"
   },
   "source": [
    "## EDA\n",
    "Let's inspect the classes and their distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e289f8c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1749570753295,
     "user": {
      "displayName": "Muthumayan Madhayyan",
      "userId": "14757534479618487320"
     },
     "user_tz": 420
    },
    "id": "e289f8c7",
    "outputId": "97d83aa1-3c1f-4e3e-9893-c455dd0c705e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets.shape: (940,)\n",
      "Unique values: [0 1 2 3 4]\n",
      "Counts: [408 234 137  93  68]\n"
     ]
    }
   ],
   "source": [
    "targets = np.array([item[0] for item in dataset])\n",
    "print(\"targets.shape:\", targets.shape)\n",
    "unique_vals, counts = np.unique(targets, return_counts=True)\n",
    "print(\"Unique values:\", unique_vals)\n",
    "print(\"Counts:\", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WmdGQOq8ICWi",
   "metadata": {
    "id": "WmdGQOq8ICWi"
   },
   "source": [
    "## Train/Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "E9IZSj2lH8qN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5528,
     "status": "ok",
     "timestamp": 1749570758826,
     "user": {
      "displayName": "Muthumayan Madhayyan",
      "userId": "14757534479618487320"
     },
     "user_tz": 420
    },
    "id": "E9IZSj2lH8qN",
    "outputId": "70758489-3025-4a29-db34-9e6801d99277"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 752\n",
      "Val size: 94\n",
      "Test size: 94\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Example: 80% train, 10% val, 10% test\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size  # handles rounding\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(63)  # for reproducibility\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Val size:\", len(val_dataset))\n",
    "print(\"Test size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "-c8BhNsc-4gH",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1749570758847,
     "user": {
      "displayName": "Muthumayan Madhayyan",
      "userId": "14757534479618487320"
     },
     "user_tz": 420
    },
    "id": "-c8BhNsc-4gH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MultiChannelDataset(Dataset):\n",
    "    def __init__(self, data, transform=None, target_transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, image = self.data[idx]\n",
    "\n",
    "        # Convert image to torch tensor (should already be in C x H x W format)\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "        # Transpose the image from (H, W, C) to (C, H, W)\n",
    "        image = image.permute(2, 0, 1) # Original shape (10, 10, 19) -> Permuted shape (19, 10, 10)\n",
    "\n",
    "        image = torch.nan_to_num(image, nan=0.0)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9-JRkNTQ-_GN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6607,
     "status": "ok",
     "timestamp": 1749570765456,
     "user": {
      "displayName": "Muthumayan Madhayyan",
      "userId": "14757534479618487320"
     },
     "user_tz": 420
    },
    "id": "9-JRkNTQ-_GN",
    "outputId": "f858982f-320a-4209-9bd1-67ad0a01e582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 2\n",
      "Image shape: torch.Size([19, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "normalize = transforms.Normalize(mean=channel_means, std=channel_stds)\n",
    "\n",
    "# Pass this to your dataset\n",
    "train_ds = MultiChannelDataset(train_dataset, transform=normalize)\n",
    "val_ds = MultiChannelDataset(val_dataset, transform=normalize)\n",
    "test_ds = MultiChannelDataset(test_dataset, transform=normalize)\n",
    "\n",
    "for image, label in train_ds:\n",
    "    print(\"Label:\", label)\n",
    "    print(\"Image shape:\", image.shape)\n",
    "    break\n",
    "\n",
    "# Extract labels\n",
    "labels = [label for _, label in train_ds]\n",
    "\n",
    "# Compute weights\n",
    "class_counts = np.bincount(labels)\n",
    "class_weights = 1. / class_counts\n",
    "sample_weights = [class_weights[label] for label in labels]\n",
    "sample_weights = torch.DoubleTensor(sample_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "LzAuizwA_UGB",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1749570765476,
     "user": {
      "displayName": "Muthumayan Madhayyan",
      "userId": "14757534479618487320"
     },
     "user_tz": 420
    },
    "id": "LzAuizwA_UGB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),  # or more for oversampling\n",
    "    replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "D4xj0BYcJ7G8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1749570765490,
     "user": {
      "displayName": "Muthumayan Madhayyan",
      "userId": "14757534479618487320"
     },
     "user_tz": 420
    },
    "id": "D4xj0BYcJ7G8",
    "outputId": "781e6562-0875-4743-d917-32b56601b802"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in train_loader: 188\n",
      "Number of batches in val_loader: 24\n",
      "Number of batches in test_loader: 24\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, sampler=sampler)\n",
    "val_loader = DataLoader(val_ds, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=4, shuffle=False)\n",
    "\n",
    "print(\"Number of batches in train_loader:\", len(train_loader))\n",
    "print(\"Number of batches in val_loader:\", len(val_loader))\n",
    "print(\"Number of batches in test_loader:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "Cyhfx1KKFdk3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1749570765496,
     "user": {
      "displayName": "Muthumayan Madhayyan",
      "userId": "14757534479618487320"
     },
     "user_tz": 420
    },
    "id": "Cyhfx1KKFdk3",
    "outputId": "00ca0e16-a2ea-421c-eb39-6258ae097d00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch class distribution: Counter({1: 2, 3: 1, 4: 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for batch_imgs, batch_labels in train_loader:\n",
    "    print(\"Batch class distribution:\", Counter(batch_labels.tolist()))\n",
    "    break  # only show first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ero0gzBSRx8B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1749570765506,
     "user": {
      "displayName": "Muthumayan Madhayyan",
      "userId": "14757534479618487320"
     },
     "user_tz": 420
    },
    "id": "ero0gzBSRx8B",
    "outputId": "a0c40260-fd56-4f0b-dced-fe6c90b023ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Load in relevant libraries, and alias where appropriate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define relevant variables for the ML task\n",
    "batch_size = 4\n",
    "num_classes = len(class_counts)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 80\n",
    "\n",
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Creating a CNN class\n",
    "class ConvNeuralNet(nn.Module):\n",
    "#  Determine what layers and their order in CNN object\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNeuralNet, self).__init__()\n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=19, out_channels=32, kernel_size=3)\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\n",
    "        self.conv_layer3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.conv_layer4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    # Progresses data across layers\n",
    "    def forward(self, x):\n",
    "        out = self.conv_layer1(x)\n",
    "#        out = self.conv_layer2(out)\n",
    "        out = self.max_pool1(out)\n",
    "\n",
    "        out = self.conv_layer3(out)\n",
    "#        out = self.conv_layer4(out)\n",
    "        out = self.max_pool2(out)\n",
    "\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7UW5rrtBSB1o",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1749570765510,
     "user": {
      "displayName": "Muthumayan Madhayyan",
      "userId": "14757534479618487320"
     },
     "user_tz": 420
    },
    "id": "7UW5rrtBSB1o"
   },
   "outputs": [],
   "source": [
    "model = ConvNeuralNet(num_classes)\n",
    "\n",
    "# Set Loss function with criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set optimizer with optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n",
    "\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "KHOX9PHUTmkg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1749570765516,
     "user": {
      "displayName": "Muthumayan Madhayyan",
      "userId": "14757534479618487320"
     },
     "user_tz": 420
    },
    "id": "KHOX9PHUTmkg",
    "outputId": "80831fd3-3ebc-44ae-cb5e-0828d8e5915c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 19, 10, 10]) tensor([0, 4, 0, 1], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "for (images, labels) in train_loader:\n",
    "    print(images.shape, labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "XEuNdMr2TTuf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36087,
     "status": "ok",
     "timestamp": 1749570801610,
     "user": {
      "displayName": "Muthumayan Madhayyan",
      "userId": "14757534479618487320"
     },
     "user_tz": 420
    },
    "id": "XEuNdMr2TTuf",
    "outputId": "418c1078-d025-49bd-e294-0552fd08bb0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Loss: 1.5797\n",
      "Epoch [2/80], Loss: 1.6456\n",
      "Epoch [3/80], Loss: 1.6484\n",
      "Epoch [4/80], Loss: 1.5695\n",
      "Epoch [5/80], Loss: 1.6769\n",
      "Epoch [6/80], Loss: 1.5599\n",
      "Epoch [7/80], Loss: 1.3875\n",
      "Epoch [8/80], Loss: 1.4186\n",
      "Epoch [9/80], Loss: 1.0619\n",
      "Epoch [10/80], Loss: 1.2547\n",
      "Epoch [11/80], Loss: 1.3362\n",
      "Epoch [12/80], Loss: 0.8851\n",
      "Epoch [13/80], Loss: 0.9381\n",
      "Epoch [14/80], Loss: 1.6843\n",
      "Epoch [15/80], Loss: 1.0105\n",
      "Epoch [16/80], Loss: 0.6743\n",
      "Epoch [17/80], Loss: 1.3848\n",
      "Epoch [18/80], Loss: 1.2539\n",
      "Epoch [19/80], Loss: 0.8807\n",
      "Epoch [20/80], Loss: 1.1220\n",
      "Epoch [21/80], Loss: 1.0023\n",
      "Epoch [22/80], Loss: 0.6582\n",
      "Epoch [23/80], Loss: 0.6458\n",
      "Epoch [24/80], Loss: 1.6909\n",
      "Epoch [25/80], Loss: 0.8088\n",
      "Epoch [26/80], Loss: 1.0710\n",
      "Epoch [27/80], Loss: 1.7402\n",
      "Epoch [28/80], Loss: 0.8579\n",
      "Epoch [29/80], Loss: 0.7316\n",
      "Epoch [30/80], Loss: 0.8093\n",
      "Epoch [31/80], Loss: 0.8647\n",
      "Epoch [32/80], Loss: 0.4441\n",
      "Epoch [33/80], Loss: 1.1850\n",
      "Epoch [34/80], Loss: 0.9384\n",
      "Epoch [35/80], Loss: 0.7988\n",
      "Epoch [36/80], Loss: 0.5891\n",
      "Epoch [37/80], Loss: 0.6117\n",
      "Epoch [38/80], Loss: 1.0700\n",
      "Epoch [39/80], Loss: 0.4320\n",
      "Epoch [40/80], Loss: 0.8539\n",
      "Epoch [41/80], Loss: 0.6719\n",
      "Epoch [42/80], Loss: 0.4577\n",
      "Epoch [43/80], Loss: 0.9994\n",
      "Epoch [44/80], Loss: 0.4987\n",
      "Epoch [45/80], Loss: 0.6263\n",
      "Epoch [46/80], Loss: 0.2006\n",
      "Epoch [47/80], Loss: 0.4062\n",
      "Epoch [48/80], Loss: 1.2476\n",
      "Epoch [49/80], Loss: 0.8340\n",
      "Epoch [50/80], Loss: 0.3549\n",
      "Epoch [51/80], Loss: 0.1308\n",
      "Epoch [52/80], Loss: 0.3972\n",
      "Epoch [53/80], Loss: 0.5798\n",
      "Epoch [54/80], Loss: 0.5111\n",
      "Epoch [55/80], Loss: 0.5052\n",
      "Epoch [56/80], Loss: 0.6501\n",
      "Epoch [57/80], Loss: 0.4740\n",
      "Epoch [58/80], Loss: 1.1469\n",
      "Epoch [59/80], Loss: 0.6573\n",
      "Epoch [60/80], Loss: 0.0735\n",
      "Epoch [61/80], Loss: 0.5963\n",
      "Epoch [62/80], Loss: 0.9282\n",
      "Epoch [63/80], Loss: 0.1235\n",
      "Epoch [64/80], Loss: 0.2973\n",
      "Epoch [65/80], Loss: 0.8707\n",
      "Epoch [66/80], Loss: 0.7247\n",
      "Epoch [67/80], Loss: 0.4269\n",
      "Epoch [68/80], Loss: 0.5206\n",
      "Epoch [69/80], Loss: 1.0876\n",
      "Epoch [70/80], Loss: 0.3618\n",
      "Epoch [71/80], Loss: 0.7420\n",
      "Epoch [72/80], Loss: 0.4795\n",
      "Epoch [73/80], Loss: 0.3278\n",
      "Epoch [74/80], Loss: 0.8621\n",
      "Epoch [75/80], Loss: 0.3738\n",
      "Epoch [76/80], Loss: 0.3893\n",
      "Epoch [77/80], Loss: 0.2727\n",
      "Epoch [78/80], Loss: 0.4889\n",
      "Epoch [79/80], Loss: 0.1545\n",
      "Epoch [80/80], Loss: 0.5169\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# We use the pre-defined number of epochs to determine how many iterations to train the network on\n",
    "for epoch in range(num_epochs):\n",
    "# Load in the data in batches using the train_loader object\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Cast labels to torch.long\n",
    "        labels = labels.to(torch.long)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "kh-LsDWkc4A5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1749570801627,
     "user": {
      "displayName": "Muthumayan Madhayyan",
      "userId": "14757534479618487320"
     },
     "user_tz": 420
    },
    "id": "kh-LsDWkc4A5",
    "outputId": "1997a7e8-9a4d-42e7-f673-fb77da487624"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on 188 batches: 83.37765957446808 %\n",
      "Accuracy of the network on 24 batches: 78.95981087470449 %\n",
      "Accuracy of the network on 24 batches: 74.8936170212766 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for which_dataset in [train_loader, val_loader, test_loader]:\n",
    "      for images, labels in which_dataset:\n",
    "          images = images.to(device)\n",
    "          labels = labels.to(device)\n",
    "          outputs = model(images)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "\n",
    "      print('Accuracy of the network on {} batches: {} %'.format(len(which_dataset), 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee239af9-070e-40db-92d3-bc9e8cdc8712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape: (940, 10, 10, 19)\n",
      "pixels.shape: (94000, 19)\n",
      "\n",
      "Channel Means (ignoring NaNs): [0.00018529 0.00197223 0.00315226 0.00513789 0.00603405 0.00401628\n",
      " 0.00471744 0.00849927 0.009139   0.00896765 0.00713521 0.00414049\n",
      " 0.0049448  0.00285523 0.00410934 0.00269009 0.00399925 0.00281253\n",
      " 0.00211597]\n",
      "Channel STDs (ignoring NaNs): [0.00519959 0.00396854 0.00399149 0.00340319 0.00370621 0.00541371\n",
      " 0.00554312 0.00466338 0.00491752 0.00480148 0.00595262 0.00486634\n",
      " 0.0040856  0.00444033 0.00357955 0.00435695 0.00340758 0.00437636\n",
      " 0.00397049]\n",
      "\n",
      "targets.shape: (940,)\n",
      "Unique values: [0 1 2 3 4]\n",
      "Counts: [408 234 137  93  68]\n",
      "\n",
      "Train size: 752\n",
      "Val size: 94\n",
      "Test size: 94\n",
      "\n",
      "Label: 2\n",
      "Image shape: torch.Size([19, 10, 10])\n",
      "\n",
      "Number of batches in train_loader: 188\n",
      "Number of batches in val_loader: 24\n",
      "Number of batches in test_loader: 24\n",
      "\n",
      "Batch class distribution: Counter({0: 1, 4: 1, 3: 1, 1: 1})\n",
      "\n",
      "cpu\n",
      "\n",
      "torch.Size([4, 19, 10, 10]) tensor([1, 2, 0, 1], dtype=torch.int32)\n",
      "\n",
      "Epoch [1/80], Loss: 1.6116\n",
      "Epoch [2/80], Loss: 1.5239\n",
      "Epoch [3/80], Loss: 1.5231\n",
      "Epoch [4/80], Loss: 1.5990\n",
      "Epoch [5/80], Loss: 1.4719\n",
      "Epoch [6/80], Loss: 0.9571\n",
      "Epoch [7/80], Loss: 1.5543\n",
      "Epoch [8/80], Loss: 1.2946\n",
      "Epoch [9/80], Loss: 1.3915\n",
      "Epoch [10/80], Loss: 1.2892\n",
      "Epoch [11/80], Loss: 1.2644\n",
      "Epoch [12/80], Loss: 1.0814\n",
      "Epoch [13/80], Loss: 1.2132\n",
      "Epoch [14/80], Loss: 1.5535\n",
      "Epoch [15/80], Loss: 1.1222\n",
      "Epoch [16/80], Loss: 1.2473\n",
      "Epoch [17/80], Loss: 1.0833\n",
      "Epoch [18/80], Loss: 1.3524\n",
      "Epoch [19/80], Loss: 1.1020\n",
      "Epoch [20/80], Loss: 0.8469\n",
      "Epoch [21/80], Loss: 1.4211\n",
      "Epoch [22/80], Loss: 0.7585\n",
      "Epoch [23/80], Loss: 1.5148\n",
      "Epoch [24/80], Loss: 1.3927\n",
      "Epoch [25/80], Loss: 1.1657\n",
      "Epoch [26/80], Loss: 1.0984\n",
      "Epoch [27/80], Loss: 1.3590\n",
      "Epoch [28/80], Loss: 0.7084\n",
      "Epoch [29/80], Loss: 0.9955\n",
      "Epoch [30/80], Loss: 0.5255\n",
      "Epoch [31/80], Loss: 1.5794\n",
      "Epoch [32/80], Loss: 1.3814\n",
      "Epoch [33/80], Loss: 1.1267\n",
      "Epoch [34/80], Loss: 1.2884\n",
      "Epoch [35/80], Loss: 0.6689\n",
      "Epoch [36/80], Loss: 0.9048\n",
      "Epoch [37/80], Loss: 0.5220\n",
      "Epoch [38/80], Loss: 0.6600\n",
      "Epoch [39/80], Loss: 1.1774\n",
      "Epoch [40/80], Loss: 0.6070\n",
      "Epoch [41/80], Loss: 0.0773\n",
      "Epoch [42/80], Loss: 0.5660\n",
      "Epoch [43/80], Loss: 0.7417\n",
      "Epoch [44/80], Loss: 0.2244\n",
      "Epoch [45/80], Loss: 0.7289\n",
      "Epoch [46/80], Loss: 0.6626\n",
      "Epoch [47/80], Loss: 0.6686\n",
      "Epoch [48/80], Loss: 0.2276\n",
      "Epoch [49/80], Loss: 0.5012\n",
      "Epoch [50/80], Loss: 0.8004\n",
      "Epoch [51/80], Loss: 0.8521\n",
      "Epoch [52/80], Loss: 0.3650\n",
      "Epoch [53/80], Loss: 0.4228\n",
      "Epoch [54/80], Loss: 0.2464\n",
      "Epoch [55/80], Loss: 0.8345\n",
      "Epoch [56/80], Loss: 0.5769\n",
      "Epoch [57/80], Loss: 0.2683\n",
      "Epoch [58/80], Loss: 1.0162\n",
      "Epoch [59/80], Loss: 0.3707\n",
      "Epoch [60/80], Loss: 0.5337\n",
      "Epoch [61/80], Loss: 0.3970\n",
      "Epoch [62/80], Loss: 0.0291\n",
      "Epoch [63/80], Loss: 0.8952\n",
      "Epoch [64/80], Loss: 1.0680\n",
      "Epoch [65/80], Loss: 0.8078\n",
      "Epoch [66/80], Loss: 0.3010\n",
      "Epoch [67/80], Loss: 0.2059\n",
      "Epoch [68/80], Loss: 0.7832\n",
      "Epoch [69/80], Loss: 0.3400\n",
      "Epoch [70/80], Loss: 0.2386\n",
      "Epoch [71/80], Loss: 0.2270\n",
      "Epoch [72/80], Loss: 0.2807\n",
      "Epoch [73/80], Loss: 0.6138\n",
      "Epoch [74/80], Loss: 0.3873\n",
      "Epoch [75/80], Loss: 0.3940\n",
      "Epoch [76/80], Loss: 0.3446\n",
      "Epoch [77/80], Loss: 0.4277\n",
      "Epoch [78/80], Loss: 0.4769\n",
      "Epoch [79/80], Loss: 0.1506\n",
      "Epoch [80/80], Loss: 0.5585\n",
      "Accuracy of the network on 188 batches: 83.37765957446808 %\n",
      "Accuracy of the network on 24 batches: 78.25059101654847 %\n",
      "Accuracy of the network on 24 batches: 74.57446808510639 %\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from collections import Counter\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split, Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "dataset = np.load('../LabelData/dataset.npy')\n",
    "\n",
    "# Step 1: Extract all image arrays\n",
    "images = np.array([item[1] for item in dataset])  # shape: (n, 10, 10, 19)\n",
    "print(\"images.shape:\", images.shape)\n",
    "\n",
    "# Step 2: Reshape to (n * 10 * 10, 19)\n",
    "pixels = images.reshape(-1, 19)\n",
    "print(\"pixels.shape:\", pixels.shape)\n",
    "print()\n",
    "\n",
    "# Step 3: Compute mean and std across all valid (non-NaN) pixels for each channel\n",
    "channel_means = np.nanmean(pixels, axis=0)\n",
    "channel_stds = np.nanstd(pixels, axis=0)\n",
    "\n",
    "print(\"Channel Means (ignoring NaNs):\", channel_means)\n",
    "print(\"Channel STDs (ignoring NaNs):\", channel_stds)\n",
    "print()\n",
    "\n",
    "targets = np.array([item[0] for item in dataset])\n",
    "print(\"targets.shape:\", targets.shape)\n",
    "unique_vals, counts = np.unique(targets, return_counts=True)\n",
    "print(\"Unique values:\", unique_vals)\n",
    "print(\"Counts:\", counts)\n",
    "print()\n",
    "\n",
    "# Example: 80% train, 10% val, 10% test\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size  # handles rounding\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(63)  # for reproducibility\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Val size:\", len(val_dataset))\n",
    "print(\"Test size:\", len(test_dataset))\n",
    "print()\n",
    "\n",
    "class MultiChannelDataset(Dataset):\n",
    "    def __init__(self, data, transform=None, target_transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, image = self.data[idx]\n",
    "\n",
    "        # Convert image to torch tensor (should already be in C x H x W format)\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "        # Transpose the image from (H, W, C) to (C, H, W)\n",
    "        image = image.permute(2, 0, 1) # Original shape (10, 10, 19) -> Permuted shape (19, 10, 10)\n",
    "\n",
    "        image = torch.nan_to_num(image, nan=0.0)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "normalize = transforms.Normalize(mean=channel_means, std=channel_stds)\n",
    "\n",
    "# Pass this to your dataset\n",
    "train_ds = MultiChannelDataset(train_dataset, transform=normalize)\n",
    "val_ds = MultiChannelDataset(val_dataset, transform=normalize)\n",
    "test_ds = MultiChannelDataset(test_dataset, transform=normalize)\n",
    "\n",
    "for image, label in train_ds:\n",
    "    print(\"Label:\", label)\n",
    "    print(\"Image shape:\", image.shape)\n",
    "    break\n",
    "print()\n",
    "\n",
    "# Extract labels\n",
    "labels = [label for _, label in train_ds]\n",
    "\n",
    "# Compute weights\n",
    "class_counts = np.bincount(labels)\n",
    "class_weights = 1. / class_counts\n",
    "sample_weights = [class_weights[label] for label in labels]\n",
    "sample_weights = torch.DoubleTensor(sample_weights)\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),  # or more for oversampling\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, sampler=sampler)\n",
    "val_loader = DataLoader(val_ds, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=4, shuffle=False)\n",
    "\n",
    "print(\"Number of batches in train_loader:\", len(train_loader))\n",
    "print(\"Number of batches in val_loader:\", len(val_loader))\n",
    "print(\"Number of batches in test_loader:\", len(test_loader))\n",
    "print()\n",
    "\n",
    "for batch_imgs, batch_labels in train_loader:\n",
    "    print(\"Batch class distribution:\", Counter(batch_labels.tolist()))\n",
    "    break  # only show first batch\n",
    "print()\n",
    "\n",
    "# Define relevant variables for the ML task\n",
    "batch_size = 4\n",
    "num_classes = len(class_counts)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 80\n",
    "\n",
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "print()\n",
    "\n",
    "# Creating a CNN class\n",
    "class ConvNeuralNet(nn.Module):\n",
    "#  Determine what layers and their order in CNN object\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNeuralNet, self).__init__()\n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=19, out_channels=32, kernel_size=3)\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\n",
    "        self.conv_layer3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.conv_layer4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    # Progresses data across layers\n",
    "    def forward(self, x):\n",
    "        out = self.conv_layer1(x)\n",
    "#        out = self.conv_layer2(out)\n",
    "        out = self.max_pool1(out)\n",
    "\n",
    "        out = self.conv_layer3(out)\n",
    "#        out = self.conv_layer4(out)\n",
    "        out = self.max_pool2(out)\n",
    "\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNeuralNet(num_classes)\n",
    "\n",
    "# Set Loss function with criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set optimizer with optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n",
    "\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for (images, labels) in train_loader:\n",
    "    print(images.shape, labels)\n",
    "    break\n",
    "print()\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# We use the pre-defined number of epochs to determine how many iterations to train the network on\n",
    "for epoch in range(num_epochs):\n",
    "# Load in the data in batches using the train_loader object\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Cast labels to torch.long\n",
    "        labels = labels.to(torch.long)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for which_dataset in [train_loader, val_loader, test_loader]:\n",
    "      for images, labels in which_dataset:\n",
    "          images = images.to(device)\n",
    "          labels = labels.to(device)\n",
    "          outputs = model(images)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "\n",
    "      print('Accuracy of the network on {} batches: {} %'.format(len(which_dataset), 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbcab36-0318-40a0-82c4-f07b85be9024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "15yvjBs_9U5-23FAr-kBBMbkdBmDtCRSL",
     "timestamp": 1749525255365
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
