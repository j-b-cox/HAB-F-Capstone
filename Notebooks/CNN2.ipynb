{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9819484-6dc1-44da-89c4-206c8d142044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape: (946, 10, 10, 19)\n",
      "pixels.shape: (94600, 19)\n",
      "\n",
      "Channel Means (ignoring NaNs): [0.00021017 0.00197592 0.00315288 0.00512588 0.00602077 0.00402063\n",
      " 0.00471909 0.00848031 0.00911741 0.00894592 0.00712974 0.00413408\n",
      " 0.0049181  0.00285458 0.00408608 0.00269061 0.00397831 0.00281257\n",
      " 0.0021146 ]\n",
      "Channel STDs (ignoring NaNs): [0.00519641 0.00396842 0.00398869 0.00341687 0.00372443 0.00538579\n",
      " 0.00551415 0.00469015 0.00494573 0.00482851 0.00592716 0.00484067\n",
      " 0.00408379 0.00441342 0.00357642 0.00432956 0.0034033  0.00434882\n",
      " 0.00394546]\n",
      "\n",
      "targets.shape: (946,)\n",
      "Unique values: [0 1 2 3 4]\n",
      "Counts: [409 238 138  93  68]\n",
      "\n",
      "Train size: 756\n",
      "Val size: 94\n",
      "Test size: 96\n",
      "\n",
      "Label: 2\n",
      "Image shape: torch.Size([19, 10, 10])\n",
      "\n",
      "Number of batches in train_loader: 189\n",
      "Number of batches in val_loader: 24\n",
      "Number of batches in test_loader: 24\n",
      "\n",
      "Batch class distribution: Counter({2: 2, 0: 1, 1: 1})\n",
      "\n",
      "cpu\n",
      "\n",
      "torch.Size([4, 19, 10, 10]) tensor([0, 1, 1, 1], dtype=torch.int32)\n",
      "\n",
      "Epoch [1/80], Loss: 1.3054\n",
      "Epoch [2/80], Loss: 1.4830\n",
      "Epoch [3/80], Loss: 1.1765\n",
      "Epoch [4/80], Loss: 1.3791\n",
      "Epoch [5/80], Loss: 1.9880\n",
      "Epoch [6/80], Loss: 1.2731\n",
      "Epoch [7/80], Loss: 1.6621\n",
      "Epoch [8/80], Loss: 1.1270\n",
      "Epoch [9/80], Loss: 1.1729\n",
      "Epoch [10/80], Loss: 1.8980\n",
      "Epoch [11/80], Loss: 1.6996\n",
      "Epoch [12/80], Loss: 1.1258\n",
      "Epoch [13/80], Loss: 0.9610\n",
      "Epoch [14/80], Loss: 1.5074\n",
      "Epoch [15/80], Loss: 1.2881\n",
      "Epoch [16/80], Loss: 1.8707\n",
      "Epoch [17/80], Loss: 0.7799\n",
      "Epoch [18/80], Loss: 0.6102\n",
      "Epoch [19/80], Loss: 0.5295\n",
      "Epoch [20/80], Loss: 0.8361\n",
      "Epoch [21/80], Loss: 0.8030\n",
      "Epoch [22/80], Loss: 0.8156\n",
      "Epoch [23/80], Loss: 0.9557\n",
      "Epoch [24/80], Loss: 0.5268\n",
      "Epoch [25/80], Loss: 0.4624\n",
      "Epoch [26/80], Loss: 0.6683\n",
      "Epoch [27/80], Loss: 1.3792\n",
      "Epoch [28/80], Loss: 0.1917\n",
      "Epoch [29/80], Loss: 2.1130\n",
      "Epoch [30/80], Loss: 1.2339\n",
      "Epoch [31/80], Loss: 1.5239\n",
      "Epoch [32/80], Loss: 0.3659\n",
      "Epoch [33/80], Loss: 0.9289\n",
      "Epoch [34/80], Loss: 0.9656\n",
      "Epoch [35/80], Loss: 0.7232\n",
      "Epoch [36/80], Loss: 0.3757\n",
      "Epoch [37/80], Loss: 0.3645\n",
      "Epoch [38/80], Loss: 1.0335\n",
      "Epoch [39/80], Loss: 0.3793\n",
      "Epoch [40/80], Loss: 0.5561\n",
      "Epoch [41/80], Loss: 0.2031\n",
      "Epoch [42/80], Loss: 0.1107\n",
      "Epoch [43/80], Loss: 0.2317\n",
      "Epoch [44/80], Loss: 0.5315\n",
      "Epoch [45/80], Loss: 0.5745\n",
      "Epoch [46/80], Loss: 2.4446\n",
      "Epoch [47/80], Loss: 0.6695\n",
      "Epoch [48/80], Loss: 0.3524\n",
      "Epoch [49/80], Loss: 1.0366\n",
      "Epoch [50/80], Loss: 0.7271\n",
      "Epoch [51/80], Loss: 0.4844\n",
      "Epoch [52/80], Loss: 0.7463\n",
      "Epoch [53/80], Loss: 0.1929\n",
      "Epoch [54/80], Loss: 0.4709\n",
      "Epoch [55/80], Loss: 0.5687\n",
      "Epoch [56/80], Loss: 0.4420\n",
      "Epoch [57/80], Loss: 0.6407\n",
      "Epoch [58/80], Loss: 0.1982\n",
      "Epoch [59/80], Loss: 0.9924\n",
      "Epoch [60/80], Loss: 0.4659\n",
      "Epoch [61/80], Loss: 0.1441\n",
      "Epoch [62/80], Loss: 0.6288\n",
      "Epoch [63/80], Loss: 0.1625\n",
      "Epoch [64/80], Loss: 0.3384\n",
      "Epoch [65/80], Loss: 0.2752\n",
      "Epoch [66/80], Loss: 0.0184\n",
      "Epoch [67/80], Loss: 0.4920\n",
      "Epoch [68/80], Loss: 0.2278\n",
      "Epoch [69/80], Loss: 0.7041\n",
      "Epoch [70/80], Loss: 0.6055\n",
      "Epoch [71/80], Loss: 0.2489\n",
      "Epoch [72/80], Loss: 0.2473\n",
      "Epoch [73/80], Loss: 0.0634\n",
      "Epoch [74/80], Loss: 0.5786\n",
      "Epoch [75/80], Loss: 0.1152\n",
      "Epoch [76/80], Loss: 0.8574\n",
      "Epoch [77/80], Loss: 0.4371\n",
      "Epoch [78/80], Loss: 0.7086\n",
      "Epoch [79/80], Loss: 1.4010\n",
      "Epoch [80/80], Loss: 0.1052\n",
      "Accuracy on train: 78.70%\n",
      "Accuracy on val  : 39.36%\n",
      "Accuracy on test : 35.42%\n",
      "🔍 Per-Class Accuracy:\n",
      "  Class 0             : Accuracy = 0.179\n",
      "  Class 1             : Accuracy = 0.821\n",
      "  Class 2             : Accuracy = 0.067\n",
      "  Class 3             : Accuracy = 0.250\n",
      "  Class 4             : Accuracy = 0.167\n",
      "\n",
      "📈 Per-Class ROC AUC:\n",
      "  Class 0             : AUC = 0.480\n",
      "  Class 1             : AUC = 0.797\n",
      "  Class 2             : AUC = 0.643\n",
      "  Class 3             : AUC = 0.609\n",
      "  Class 4             : AUC = 0.699\n",
      "[[ 7 14  6  8  4]\n",
      " [ 1 23  1  1  2]\n",
      " [ 2  7  1  4  1]\n",
      " [ 3  2  1  2  0]\n",
      " [ 1  3  1  0  1]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from collections import Counter\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split, Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # Slower, more reproducible\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(63)\n",
    "\n",
    "\n",
    "dataset = np.load('../LabelData/dataset.npy')\n",
    "\n",
    "# Step 1: Extract all image arrays\n",
    "images = np.array([item[1] for item in dataset])  # shape: (n, 10, 10, 19)\n",
    "print(\"images.shape:\", images.shape)\n",
    "\n",
    "# Step 2: Reshape to (n * 10 * 10, 19)\n",
    "pixels = images.reshape(-1, 19)\n",
    "print(\"pixels.shape:\", pixels.shape)\n",
    "print()\n",
    "\n",
    "# Step 3: Compute mean and std across all valid (non-NaN) pixels for each channel\n",
    "channel_means = np.nanmean(pixels, axis=0)\n",
    "channel_stds = np.nanstd(pixels, axis=0)\n",
    "\n",
    "print(\"Channel Means (ignoring NaNs):\", channel_means)\n",
    "print(\"Channel STDs (ignoring NaNs):\", channel_stds)\n",
    "print()\n",
    "\n",
    "targets = np.array([item[0] for item in dataset])\n",
    "print(\"targets.shape:\", targets.shape)\n",
    "unique_vals, counts = np.unique(targets, return_counts=True)\n",
    "print(\"Unique values:\", unique_vals)\n",
    "print(\"Counts:\", counts)\n",
    "print()\n",
    "\n",
    "# Example: 80% train, 10% val, 10% test\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size  # handles rounding\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(63)\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Val size:\", len(val_dataset))\n",
    "print(\"Test size:\", len(test_dataset))\n",
    "print()\n",
    "\n",
    "class MultiChannelDataset(Dataset):\n",
    "    def __init__(self, data, transform=None, target_transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, image = self.data[idx]\n",
    "\n",
    "        # Convert image to torch tensor (should already be in C x H x W format)\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "        # Transpose the image from (H, W, C) to (C, H, W)\n",
    "        image = image.permute(2, 0, 1) # Original shape (10, 10, 19) -> Permuted shape (19, 10, 10)\n",
    "\n",
    "        image = torch.nan_to_num(image, nan=0.0)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "normalize = transforms.Normalize(mean=channel_means, std=channel_stds)\n",
    "\n",
    "# Pass this to your dataset\n",
    "train_ds = MultiChannelDataset(train_dataset, transform=normalize)\n",
    "val_ds = MultiChannelDataset(val_dataset, transform=normalize)\n",
    "test_ds = MultiChannelDataset(test_dataset, transform=normalize)\n",
    "\n",
    "for image, label in train_ds:\n",
    "    print(\"Label:\", label)\n",
    "    print(\"Image shape:\", image.shape)\n",
    "    break\n",
    "print()\n",
    "\n",
    "# Extract labels\n",
    "labels = [label for _, label in train_ds]\n",
    "\n",
    "# Compute weights\n",
    "class_counts = np.bincount(labels)\n",
    "class_weights = 1. / class_counts\n",
    "sample_weights = [class_weights[label] for label in labels]\n",
    "sample_weights = torch.DoubleTensor(sample_weights)\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),  # or more for oversampling\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, sampler=sampler)\n",
    "val_loader = DataLoader(val_ds, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=4, shuffle=False)\n",
    "\n",
    "print(\"Number of batches in train_loader:\", len(train_loader))\n",
    "print(\"Number of batches in val_loader:\", len(val_loader))\n",
    "print(\"Number of batches in test_loader:\", len(test_loader))\n",
    "print()\n",
    "\n",
    "for batch_imgs, batch_labels in train_loader:\n",
    "    print(\"Batch class distribution:\", Counter(batch_labels.tolist()))\n",
    "    break  # only show first batch\n",
    "print()\n",
    "\n",
    "# Define relevant variables for the ML task\n",
    "batch_size = 4\n",
    "num_classes = len(class_counts)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 80\n",
    "\n",
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "print()\n",
    "\n",
    "# Creating a CNN class\n",
    "class ConvNeuralNet(nn.Module):\n",
    "#  Determine what layers and their order in CNN object\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNeuralNet, self).__init__()\n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=19, out_channels=32, kernel_size=3)\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\n",
    "        self.conv_layer3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.conv_layer4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    # Progresses data across layers\n",
    "    def forward(self, x):\n",
    "        out = self.conv_layer1(x)\n",
    "#        out = self.conv_layer2(out)\n",
    "        out = self.max_pool1(out)\n",
    "\n",
    "        out = self.conv_layer3(out)\n",
    "#        out = self.conv_layer4(out)\n",
    "        out = self.max_pool2(out)\n",
    "\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNeuralNet(num_classes)\n",
    "\n",
    "# Set Loss function with criterion\n",
    "class_weights_tensor = torch.tensor(1. / class_counts, dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "# Set optimizer with optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n",
    "\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for (images, labels) in train_loader:\n",
    "    print(images.shape, labels)\n",
    "    break\n",
    "print()\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# We use the pre-defined number of epochs to determine how many iterations to train the network on\n",
    "for epoch in range(num_epochs):\n",
    "# Load in the data in batches using the train_loader object\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Cast labels to torch.long\n",
    "        labels = labels.to(torch.long)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "with torch.no_grad():\n",
    "    dataset_names = ['train', 'val', 'test']\n",
    "    loaders       = [train_loader, val_loader, test_loader]\n",
    "\n",
    "    for name, loader in zip(dataset_names, loaders):\n",
    "        correct, total = 0, 0\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = outputs.max(1)\n",
    "            total   += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "        acc = 100 * correct / total\n",
    "        print(f'Accuracy on {name:5s}: {acc:5.2f}%')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, dataloader, num_classes, device, class_names=None):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(probs, 1)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_probs = np.array(y_probs)\n",
    "\n",
    "    print(\"🔍 Per-Class Accuracy:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "    for i, acc in enumerate(per_class_acc):\n",
    "        name = class_names[i] if class_names else f\"Class {i}\"\n",
    "        print(f\"  {name:<20}: Accuracy = {acc:.3f}\")\n",
    "\n",
    "    print(\"\\n📈 Per-Class ROC AUC:\")\n",
    "    try:\n",
    "        y_true_bin = label_binarize(y_true, classes=list(range(num_classes)))\n",
    "        for i in range(num_classes):\n",
    "            auc = roc_auc_score(y_true_bin[:, i], y_probs[:, i])\n",
    "            name = class_names[i] if class_names else f\"Class {i}\"\n",
    "            print(f\"  {name:<20}: AUC = {auc:.3f}\")\n",
    "    except ValueError as e:\n",
    "        print(\"ROC AUC computation failed:\", e)\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "evaluate_model(model, test_loader, num_classes=num_classes, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d893f06-5674-45df-b1a0-004b70d4ccd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Class Counts: [330 183 111  74  58]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Class Counts:\", np.bincount([lbl for _, lbl in train_ds]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e7aa6d-d131-44d2-aa30-81667efa222a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e14100e-0e72-4f76-9d08-3f7807ea2ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
