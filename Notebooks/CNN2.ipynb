{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9819484-6dc1-44da-89c4-206c8d142044",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../LabelData/dataset.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     23\u001b[39m     torch.backends.cudnn.benchmark = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     25\u001b[39m seed_everything(\u001b[32m63\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m dataset = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../LabelData/dataset.npy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Step 1: Extract all image arrays\u001b[39;00m\n\u001b[32m     31\u001b[39m images = np.array([item[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m dataset])  \u001b[38;5;66;03m# shape: (n, 10, 10, 19)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/numpy/lib/_npyio_impl.py:459\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    457\u001b[39m     own_fid = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     fid = stack.enter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    460\u001b[39m     own_fid = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    462\u001b[39m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../LabelData/dataset.npy'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from collections import Counter\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split, Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # Slower, more reproducible\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(63)\n",
    "\n",
    "\n",
    "dataset = np.load('../LabelData/dataset.npy')\n",
    "\n",
    "# Step 1: Extract all image arrays\n",
    "images = np.array([item[1] for item in dataset])  # shape: (n, 10, 10, 19)\n",
    "print(\"images.shape:\", images.shape)\n",
    "\n",
    "# Step 2: Reshape to (n * 10 * 10, 19)\n",
    "pixels = images.reshape(-1, 19)\n",
    "print(\"pixels.shape:\", pixels.shape)\n",
    "print()\n",
    "\n",
    "# Step 3: Compute mean and std across all valid (non-NaN) pixels for each channel\n",
    "channel_means = np.nanmean(pixels, axis=0)\n",
    "channel_stds = np.nanstd(pixels, axis=0)\n",
    "\n",
    "print(\"Channel Means (ignoring NaNs):\", channel_means)\n",
    "print(\"Channel STDs (ignoring NaNs):\", channel_stds)\n",
    "print()\n",
    "\n",
    "targets = np.array([item[0] for item in dataset])\n",
    "print(\"targets.shape:\", targets.shape)\n",
    "unique_vals, counts = np.unique(targets, return_counts=True)\n",
    "print(\"Unique values:\", unique_vals)\n",
    "print(\"Counts:\", counts)\n",
    "print()\n",
    "\n",
    "# Example: 80% train, 10% val, 10% test\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size  # handles rounding\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(63)\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Val size:\", len(val_dataset))\n",
    "print(\"Test size:\", len(test_dataset))\n",
    "print()\n",
    "\n",
    "class MultiChannelDataset(Dataset):\n",
    "    def __init__(self, data, transform=None, target_transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, image = self.data[idx]\n",
    "\n",
    "        # Convert image to torch tensor (should already be in C x H x W format)\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "        # Transpose the image from (H, W, C) to (C, H, W)\n",
    "        image = image.permute(2, 0, 1) # Original shape (10, 10, 19) -> Permuted shape (19, 10, 10)\n",
    "\n",
    "        image = torch.nan_to_num(image, nan=0.0)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "normalize = transforms.Normalize(mean=channel_means, std=channel_stds)\n",
    "\n",
    "# Pass this to your dataset\n",
    "train_ds = MultiChannelDataset(train_dataset, transform=normalize)\n",
    "val_ds = MultiChannelDataset(val_dataset, transform=normalize)\n",
    "test_ds = MultiChannelDataset(test_dataset, transform=normalize)\n",
    "\n",
    "for image, label in train_ds:\n",
    "    print(\"Label:\", label)\n",
    "    print(\"Image shape:\", image.shape)\n",
    "    break\n",
    "print()\n",
    "\n",
    "# Extract labels\n",
    "labels = [label for _, label in train_ds]\n",
    "\n",
    "# Compute weights\n",
    "class_counts = np.bincount(labels)\n",
    "class_weights = 1. / class_counts\n",
    "sample_weights = [class_weights[label] for label in labels]\n",
    "sample_weights = torch.DoubleTensor(sample_weights)\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),  # or more for oversampling\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, sampler=sampler)\n",
    "val_loader = DataLoader(val_ds, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=4, shuffle=False)\n",
    "\n",
    "print(\"Number of batches in train_loader:\", len(train_loader))\n",
    "print(\"Number of batches in val_loader:\", len(val_loader))\n",
    "print(\"Number of batches in test_loader:\", len(test_loader))\n",
    "print()\n",
    "\n",
    "for batch_imgs, batch_labels in train_loader:\n",
    "    print(\"Batch class distribution:\", Counter(batch_labels.tolist()))\n",
    "    break  # only show first batch\n",
    "print()\n",
    "\n",
    "# Define relevant variables for the ML task\n",
    "batch_size = 4\n",
    "num_classes = len(class_counts)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 80\n",
    "\n",
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "print()\n",
    "\n",
    "# Creating a CNN class\n",
    "class ConvNeuralNet(nn.Module):\n",
    "#  Determine what layers and their order in CNN object\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNeuralNet, self).__init__()\n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=19, out_channels=32, kernel_size=3)\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\n",
    "        self.conv_layer3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.conv_layer4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    # Progresses data across layers\n",
    "    def forward(self, x):\n",
    "        out = self.conv_layer1(x)\n",
    "#        out = self.conv_layer2(out)\n",
    "        out = self.max_pool1(out)\n",
    "\n",
    "        out = self.conv_layer3(out)\n",
    "#        out = self.conv_layer4(out)\n",
    "        out = self.max_pool2(out)\n",
    "\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNeuralNet(num_classes)\n",
    "\n",
    "# Set Loss function with criterion\n",
    "class_weights_tensor = torch.tensor(1. / class_counts, dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "# Set optimizer with optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n",
    "\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for (images, labels) in train_loader:\n",
    "    print(images.shape, labels)\n",
    "    break\n",
    "print()\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# We use the pre-defined number of epochs to determine how many iterations to train the network on\n",
    "for epoch in range(num_epochs):\n",
    "# Load in the data in batches using the train_loader object\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Cast labels to torch.long\n",
    "        labels = labels.to(torch.long)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "with torch.no_grad():\n",
    "    dataset_names = ['train', 'val', 'test']\n",
    "    loaders       = [train_loader, val_loader, test_loader]\n",
    "\n",
    "    for name, loader in zip(dataset_names, loaders):\n",
    "        correct, total = 0, 0\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = outputs.max(1)\n",
    "            total   += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "        acc = 100 * correct / total\n",
    "        print(f'Accuracy on {name:5s}: {acc:5.2f}%')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, dataloader, num_classes, device, class_names=None):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(probs, 1)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_probs = np.array(y_probs)\n",
    "\n",
    "    print(\"🔍 Per-Class Accuracy:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "    for i, acc in enumerate(per_class_acc):\n",
    "        name = class_names[i] if class_names else f\"Class {i}\"\n",
    "        print(f\"  {name:<20}: Accuracy = {acc:.3f}\")\n",
    "\n",
    "    print(\"\\n📈 Per-Class ROC AUC:\")\n",
    "    try:\n",
    "        y_true_bin = label_binarize(y_true, classes=list(range(num_classes)))\n",
    "        for i in range(num_classes):\n",
    "            auc = roc_auc_score(y_true_bin[:, i], y_probs[:, i])\n",
    "            name = class_names[i] if class_names else f\"Class {i}\"\n",
    "            print(f\"  {name:<20}: AUC = {auc:.3f}\")\n",
    "    except ValueError as e:\n",
    "        print(\"ROC AUC computation failed:\", e)\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "evaluate_model(model, test_loader, num_classes=num_classes, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d893f06-5674-45df-b1a0-004b70d4ccd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Class Counts: [330 183 111  74  58]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Class Counts:\", np.bincount([lbl for _, lbl in train_ds]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e7aa6d-d131-44d2-aa30-81667efa222a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e14100e-0e72-4f76-9d08-3f7807ea2ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
