{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72afc575-f558-4216-8050-43d4656bffbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving wavelength list from a reference file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|████████████████████████| 9/9 [00:00<00:00, 3115.35it/s]\n",
      "PROCESSING TASKS | : 100%|█████████████████████| 9/9 [00:00<00:00, 29308.02it/s]\n",
      "COLLECTING RESULTS | : 100%|███████████████████| 9/9 [00:00<00:00, 49474.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 172 channels.\n",
      "\n",
      "Processing 2024-04-14 (composite window 2024-04-07 to 2024-04-13)\n",
      "Need to download 10 new files for this window.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|██████████████████████| 10/10 [00:00<00:00, 1127.38it/s]\n",
      "PROCESSING TASKS | : 100%|██████████████████████| 10/10 [00:29<00:00,  2.92s/it]\n",
      "COLLECTING RESULTS | : 100%|█████████████████| 10/10 [00:00<00:00, 67541.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached data for PACE_OCI.20240407T174325.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240408T181826.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240409T171508.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240409T185328.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240410T175010.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240411T164652.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240411T182512.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240412T172154.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240412T190014.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240413T175656.L2.OC_AOP.V3_0.nc\n",
      "Deleted old file: PACE_OCI.20240407T174325.L2.OC_AOP.V3_0.nc\n",
      "\n",
      "Processing 2024-04-15 (composite window 2024-04-08 to 2024-04-14)\n",
      "Need to download 11 new files for this window.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|██████████████████████| 11/11 [00:00<00:00, 3548.48it/s]\n",
      "PROCESSING TASKS | : 100%|███████████████████| 11/11 [00:00<00:00, 24831.72it/s]\n",
      "COLLECTING RESULTS | : 100%|█████████████████| 11/11 [00:00<00:00, 12661.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached data for PACE_OCI.20240408T181826.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240409T171508.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240409T185328.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240410T175010.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240411T164652.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240411T182512.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240412T172154.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240412T190014.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240413T175656.L2.OC_AOP.V3_0.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached data for PACE_OCI.20240414T165338.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240414T183158.L2.OC_AOP.V3_0.nc\n",
      "Deleted old file: PACE_OCI.20240408T181826.L2.OC_AOP.V3_0.nc\n",
      "\n",
      "Processing 2024-04-16 (composite window 2024-04-09 to 2024-04-15)\n",
      "Need to download 12 new files for this window.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|██████████████████████| 12/12 [00:00<00:00, 2776.00it/s]\n",
      "PROCESSING TASKS | : 100%|███████████████████| 12/12 [00:00<00:00, 38216.89it/s]\n",
      "COLLECTING RESULTS | : 100%|████████████████| 12/12 [00:00<00:00, 134576.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached data for PACE_OCI.20240409T171508.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240409T185328.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240410T175010.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240411T164652.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240411T182512.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240412T172154.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240412T190014.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240413T175656.L2.OC_AOP.V3_0.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached data for PACE_OCI.20240414T165338.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240414T183158.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240415T172837.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240415T190657.L2.OC_AOP.V3_0.nc\n",
      "Deleted old file: PACE_OCI.20240409T185328.L2.OC_AOP.V3_0.nc\n",
      "Deleted old file: PACE_OCI.20240409T171508.L2.OC_AOP.V3_0.nc\n",
      "\n",
      "Processing 2024-04-17 (composite window 2024-04-10 to 2024-04-16)\n",
      "Need to download 11 new files for this window.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|██████████████████████| 11/11 [00:00<00:00, 2595.05it/s]\n",
      "PROCESSING TASKS | : 100%|███████████████████| 11/11 [00:00<00:00, 75387.82it/s]\n",
      "COLLECTING RESULTS | : 100%|█████████████████| 11/11 [00:00<00:00, 28339.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached data for PACE_OCI.20240410T175010.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240411T164652.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240411T182512.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240412T172154.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240412T190014.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240413T175656.L2.OC_AOP.V3_0.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached data for PACE_OCI.20240414T165338.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240414T183158.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240415T172837.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240415T190657.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240416T180338.L2.OC_AOP.V3_0.nc\n",
      "Deleted old file: PACE_OCI.20240410T175010.L2.OC_AOP.V3_0.nc\n",
      "\n",
      "Processing 2024-04-18 (composite window 2024-04-11 to 2024-04-17)\n",
      "Need to download 12 new files for this window.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|██████████████████████| 12/12 [00:00<00:00, 2993.79it/s]\n",
      "PROCESSING TASKS | : 100%|███████████████████| 12/12 [00:00<00:00, 78398.21it/s]\n",
      "COLLECTING RESULTS | : 100%|████████████████| 12/12 [00:00<00:00, 155344.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached data for PACE_OCI.20240411T164652.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240411T182512.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240412T172154.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240412T190014.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240413T175656.L2.OC_AOP.V3_0.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached data for PACE_OCI.20240414T165338.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240414T183158.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240415T172837.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240415T190657.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240416T180338.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240417T170019.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240417T183839.L2.OC_AOP.V3_0.nc\n",
      "Deleted old file: PACE_OCI.20240411T164652.L2.OC_AOP.V3_0.nc\n",
      "Deleted old file: PACE_OCI.20240411T182512.L2.OC_AOP.V3_0.nc\n",
      "\n",
      "Processing 2024-04-19 (composite window 2024-04-12 to 2024-04-18)\n",
      "Need to download 11 new files for this window.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|██████████████████████| 11/11 [00:00<00:00, 2907.75it/s]\n",
      "PROCESSING TASKS | : 100%|███████████████████| 11/11 [00:00<00:00, 34379.54it/s]\n",
      "COLLECTING RESULTS | : 100%|█████████████████| 11/11 [00:00<00:00, 68250.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached data for PACE_OCI.20240412T172154.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240412T190014.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240413T175656.L2.OC_AOP.V3_0.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached data for PACE_OCI.20240414T165338.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240414T183158.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240415T172837.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240415T190657.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240416T180338.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240417T170019.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240417T183839.L2.OC_AOP.V3_0.nc\n",
      "Using cached data for PACE_OCI.20240418T173520.L2.OC_AOP.V3_0.nc\n",
      "Deleted old file: PACE_OCI.20240412T190014.L2.OC_AOP.V3_0.nc\n",
      "Deleted old file: PACE_OCI.20240412T172154.L2.OC_AOP.V3_0.nc\n",
      "\n",
      "Processing 2024-04-20 (composite window 2024-04-13 to 2024-04-19)\n",
      "Need to download 10 new files for this window.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|██████████████████████| 10/10 [00:00<00:00, 3425.88it/s]\n",
      "PROCESSING TASKS | : 100%|███████████████████| 10/10 [00:00<00:00, 44858.87it/s]\n",
      "COLLECTING RESULTS | : 100%|█████████████████| 10/10 [00:00<00:00, 71943.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached data for PACE_OCI.20240413T175656.L2.OC_AOP.V3_0.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line -1\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import earthaccess\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Authenticate\n",
    "auth = earthaccess.login(persist=True)\n",
    "\n",
    "# Parameters\n",
    "bbox = (-83.62, 41.34, -82, 42.27)      # (lon_min, lat_min, lon_max, lat_max)\n",
    "res = 0.01\n",
    "start_date = datetime(2024, 4, 14)\n",
    "end_date = datetime(2025, 5, 23)\n",
    "window_size = 7\n",
    "decay = 0.8\n",
    "\n",
    "# Ensure data directories exist\n",
    "os.makedirs(\"../Data/\", exist_ok=True)\n",
    "os.makedirs(\"../Cache/\", exist_ok=True)\n",
    "os.makedirs(\"../Images/\", exist_ok=True)\n",
    "\n",
    "# Output grid\n",
    "lat_bins = np.arange(bbox[1], bbox[3] + res, res)\n",
    "lon_bins = np.arange(bbox[0], bbox[2] + res, res)\n",
    "lat_centers = 0.5 * (lat_bins[:-1] + lat_bins[1:])\n",
    "lon_centers = 0.5 * (lon_bins[:-1] + lon_bins[1:])\n",
    "nlat, nlon = len(lat_centers), len(lon_centers)\n",
    "\n",
    "# --- Retry Helpers ---\n",
    "\n",
    "def safe_search(short_name, temporal, bounding_box, max_retries=500):\n",
    "    retries = 0\n",
    "    while True:\n",
    "        try:\n",
    "            results = earthaccess.search_data(\n",
    "                short_name=short_name,\n",
    "                temporal=temporal,\n",
    "                bounding_box=bounding_box\n",
    "            )\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            if retries >= max_retries:\n",
    "                print(f\"Search failed after {max_retries} retries: {e}\")\n",
    "                return []\n",
    "            wait = 5 + random.uniform(0, 3)\n",
    "            print(f\"Search error: {e}. Retrying in {wait:.1f} seconds...\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "def safe_download(results, directory=\"../Data/\", max_retries=5):\n",
    "    retries = 0\n",
    "    while True:\n",
    "        try:\n",
    "            paths = earthaccess.download(results, directory)\n",
    "            return paths\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            if retries >= max_retries:\n",
    "                print(f\"Download failed after {max_retries} retries: {e}\")\n",
    "                return []\n",
    "            wait = 5 + random.uniform(0, 3)\n",
    "            print(f\"Download error: {e}. Retrying in {wait:.1f} seconds...\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "# Wavelengths from a reference file\n",
    "print(\"Retrieving wavelength list from a reference file...\")\n",
    "search_ref = safe_search(\n",
    "    short_name=\"PACE_OCI_L2_AOP\",\n",
    "    temporal=(\"2024-06-01\", \"2024-06-05\"),\n",
    "    bounding_box=bbox,\n",
    ")\n",
    "if not search_ref:\n",
    "    raise RuntimeError(\"No reference files found to retrieve wavelengths.\")\n",
    "ref_file = safe_download(search_ref, \"../Data/\")[0]\n",
    "wave_all = xr.open_dataset(ref_file, group=\"sensor_band_parameters\")[\"wavelength_3d\"].data\n",
    "num_channels = len(wave_all)\n",
    "print(f\"Found {num_channels} channels.\")\n",
    "\n",
    "# Prepare main array\n",
    "total_days = (end_date - start_date).days + 1\n",
    "ndarray_all = np.full((total_days, nlat, nlon, num_channels), np.nan, dtype=np.float32)\n",
    "\n",
    "# Process day by day\n",
    "for day_idx in range(total_days):\n",
    "    current_date = start_date + timedelta(days=day_idx)\n",
    "    window_start = current_date - timedelta(days=window_size)\n",
    "    window_end = current_date - timedelta(days=1)\n",
    "\n",
    "    print(f\"\\nProcessing {current_date.date()} (composite window {window_start.date()} to {window_end.date()})\")\n",
    "\n",
    "    # Search for data in window (with retry)\n",
    "    results = safe_search(\n",
    "        short_name=\"PACE_OCI_L2_AOP\",\n",
    "        temporal=(window_start.strftime(\"%Y-%m-%d\"), window_end.strftime(\"%Y-%m-%d\")),\n",
    "        bounding_box=bbox,\n",
    "    )\n",
    "\n",
    "    if not results:\n",
    "        print(\"No data found for this window. Skipping to next date.\")\n",
    "        continue\n",
    "\n",
    "    # Download files (with retry)\n",
    "    # Determine which files need downloading (those not yet cached)\n",
    "    to_download = []\n",
    "    for granule in results:\n",
    "        granule_id = granule['umm']['DataGranule']['ArchiveAndDistributionInformation'][0]['Name']\n",
    "        granule_basename = os.path.basename(granule_id)  # or appropriate key for ID\n",
    "        cache_path = f\"../Cache/{granule_basename}.nc.npz\"\n",
    "        if not os.path.exists(cache_path):\n",
    "            to_download.append(granule)\n",
    "    \n",
    "    if to_download:\n",
    "        print(f\"Need to download {len(to_download)} new files for this window.\")\n",
    "        paths = safe_download(to_download, \"../Data/\")\n",
    "        if not paths:\n",
    "            print(\"No files downloaded for this window. Skipping to next date.\")\n",
    "            continue\n",
    "    else:\n",
    "        print(\"All files already cached. Skipping download.\")\n",
    "        paths = [f\"../Data/{os.path.basename(granule['granule_id'])}\" for granule in results]\n",
    "\n",
    "\n",
    "    # Initialize sum and weight arrays\n",
    "    sum_all = np.zeros((num_channels, nlat, nlon))\n",
    "    weight_all = np.zeros((num_channels, nlat, nlon))\n",
    "\n",
    "    for path in paths:\n",
    "        base = os.path.basename(path)\n",
    "        date_str = base.split(\".\")[1][:8]\n",
    "        file_date = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "        delta_days = (window_end - file_date).days\n",
    "        weight = decay ** delta_days if delta_days >= 0 else 0\n",
    "\n",
    "        cache_file = f\"../Cache/{base}.npz\"\n",
    "\n",
    "        if not os.path.exists(cache_file):\n",
    "            print(f\"Processing {base} (not in cache)\")\n",
    "            try:\n",
    "                nav = xr.open_dataset(path, group=\"navigation_data\")\n",
    "                lat = nav[\"latitude\"].values\n",
    "                lon = nav[\"longitude\"].values\n",
    "\n",
    "                rrs_ds = xr.open_dataset(path, group=\"geophysical_data\")[\"Rrs\"]\n",
    "                rrs_ds = rrs_ds.assign_coords(wavelength_3d=wave_all)\n",
    "\n",
    "                lat_idx_all = []\n",
    "                lon_idx_all = []\n",
    "                ch_idx_all = []\n",
    "                val_all = []\n",
    "\n",
    "                for ch_idx, wl in tqdm(list(enumerate(wave_all)), desc=f\"Channels in {base}\", leave=False):\n",
    "                    band = rrs_ds.sel(wavelength_3d=wl, method=\"nearest\").values\n",
    "                    mask = (\n",
    "                        np.isfinite(band) &\n",
    "                        (lat >= bbox[1]) & (lat <= bbox[3]) &\n",
    "                        (lon >= bbox[0]) & (lon <= bbox[2])\n",
    "                    )\n",
    "                    lat_valid = lat[mask]\n",
    "                    lon_valid = lon[mask]\n",
    "                    val_valid = band[mask]\n",
    "\n",
    "                    lat_idx = np.searchsorted(lat_bins, lat_valid) - 1\n",
    "                    lon_idx = np.searchsorted(lon_bins, lon_valid) - 1\n",
    "\n",
    "                    lat_idx_all.extend(lat_idx)\n",
    "                    lon_idx_all.extend(lon_idx)\n",
    "                    ch_idx_all.extend([ch_idx] * len(val_valid))\n",
    "                    val_all.extend(val_valid)\n",
    "\n",
    "                np.savez_compressed(cache_file,\n",
    "                                    lat_idx=np.array(lat_idx_all, dtype=np.int16),\n",
    "                                    lon_idx=np.array(lon_idx_all, dtype=np.int16),\n",
    "                                    ch_idx=np.array(ch_idx_all, dtype=np.int16),\n",
    "                                    val=np.array(val_all, dtype=np.float32))\n",
    "                print(f\"Cached data to {cache_file}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {path}: {e}\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"Using cached data for {base}\")\n",
    "\n",
    "        # Load from cache\n",
    "        data = np.load(cache_file)\n",
    "        lat_idx = data['lat_idx']\n",
    "        lon_idx = data['lon_idx']\n",
    "        ch_idx = data['ch_idx']\n",
    "        val = data['val']\n",
    "\n",
    "        for j in range(len(val)):\n",
    "            if 0 <= lat_idx[j] < nlat and 0 <= lon_idx[j] < nlon:\n",
    "                sum_all[ch_idx[j], lat_idx[j], lon_idx[j]] += val[j] * weight\n",
    "                weight_all[ch_idx[j], lat_idx[j], lon_idx[j]] += weight\n",
    "\n",
    "    # Finalize average\n",
    "    with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
    "        avg_all = sum_all / weight_all\n",
    "        avg_all[weight_all == 0] = np.nan\n",
    "\n",
    "    ndarray_all[day_idx] = np.transpose(avg_all, (1, 2, 0))\n",
    "\n",
    "    # Delete old files\n",
    "    delete_date = (current_date - timedelta(days=window_size)).strftime('%Y%m%d')\n",
    "    for fname in os.listdir(\"../Data/\"):\n",
    "        if delete_date in fname and fname.endswith(\".nc\"):\n",
    "            try:\n",
    "                os.remove(os.path.join(\"../Data/\", fname))\n",
    "                print(f\"Deleted old file: {fname}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not delete {fname}: {e}\")\n",
    "\n",
    "# Save results\n",
    "np.save(\"../Images/composite_data.npy\", ndarray_all)\n",
    "print(\"\\nSaved full 4D composite data array to '../Images/composite_data.npy'.\")\n",
    "\n",
    "metadata = {\"wavelengths\": wave_all, \"lat\": lat_centers, \"lon\": lon_centers}\n",
    "with open(\"../Images/composite_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(\"Saved metadata (wavelengths, lat, lon) to '../Images/composite_metadata.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01a4a7e2-29fb-454b-8cf4-22533d4dd10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First granule keys:\n",
      "dict_keys(['TemporalExtent', 'GranuleUR', 'SpatialExtent', 'ProviderDates', 'CollectionReference', 'RelatedUrls', 'CloudCover', 'DataGranule', 'Platforms', 'MetadataSpecification'])\n",
      "\n",
      "First granule example:\n",
      "{'CloudCover': 48.2,\n",
      " 'CollectionReference': {'ShortName': 'PACE_OCI_L2_AOP', 'Version': '3.0'},\n",
      " 'DataGranule': {'ArchiveAndDistributionInformation': [{'Format': 'netCDF-4',\n",
      "                                                        'MimeType': 'application/x-netcdf',\n",
      "                                                        'Name': 'PACE_OCI.20240601T165222.L2.OC_AOP.V3_0.nc',\n",
      "                                                        'SizeInBytes': 397593763}],\n",
      "                 'DayNightFlag': 'Day',\n",
      "                 'Identifiers': [{'Identifier': 'PACE_OCI.20240601T165222.L2.OC_AOP.V3_0.nc',\n",
      "                                  'IdentifierType': 'ProducerGranuleId'}],\n",
      "                 'ProductionDateTime': '2025-02-10T19:48:27.503Z'},\n",
      " 'GranuleUR': 'PACE_OCI_L2_AOP_PACE_OCI.20240601T165222.L2.OC_AOP.V3_0.nc_3.0',\n",
      " 'MetadataSpecification': {'Name': 'UMM-G',\n",
      "                           'URL': 'https://cdn.earthdata.nasa.gov/umm/granule/v1.6.6',\n",
      "                           'Version': '1.6.6'},\n",
      " 'Platforms': [{'Instruments': [{'ShortName': 'OCI'}], 'ShortName': 'PACE'}],\n",
      " 'ProviderDates': [{'Date': '2025-02-10T20:02:55Z', 'Type': 'Insert'},\n",
      "                   {'Date': '2025-02-10T19:48:27.503Z', 'Type': 'Update'}],\n",
      " 'RelatedUrls': [{'Format': 'PNG',\n",
      "                  'MimeType': 'image/png',\n",
      "                  'Type': 'GET RELATED VISUALIZATION',\n",
      "                  'URL': 'https://oceandata.sci.gsfc.nasa.gov/browse_images/PACE_OCI.20240601T165222.L2.OC_AOP.V3_0.nc.png'},\n",
      "                 {'Description': 'Download '\n",
      "                                 'PACE_OCI.20240601T165222.L2.OC_AOP.V3_0.nc',\n",
      "                  'Type': 'GET DATA',\n",
      "                  'URL': 'https://obdaac-tea.earthdatacloud.nasa.gov/ob-cumulus-prod-public/PACE_OCI.20240601T165222.L2.OC_AOP.V3_0.nc'},\n",
      "                 {'Description': 'This link provides direct download access '\n",
      "                                 'via S3 to the granule',\n",
      "                  'Type': 'GET DATA VIA DIRECT ACCESS',\n",
      "                  'URL': 's3://ob-cumulus-prod-public/PACE_OCI.20240601T165222.L2.OC_AOP.V3_0.nc'},\n",
      "                 {'Description': 'api endpoint to retrieve temporary '\n",
      "                                 'credentials valid for same-region direct s3 '\n",
      "                                 'access',\n",
      "                  'Type': 'VIEW RELATED INFORMATION',\n",
      "                  'URL': 'https://obdaac-tea.earthdatacloud.nasa.gov/s3credentials'}],\n",
      " 'SpatialExtent': {'HorizontalSpatialDomain': {'Geometry': {'GPolygons': [{'Boundary': {'Points': [{'Latitude': 48.07948,\n",
      "                                                                                                    'Longitude': -50.50789},\n",
      "                                                                                                   {'Latitude': 42.0875,\n",
      "                                                                                                    'Longitude': -84.03094},\n",
      "                                                                                                   {'Latitude': 24.9461,\n",
      "                                                                                                    'Longitude': -75.73668},\n",
      "                                                                                                   {'Latitude': 30.2528,\n",
      "                                                                                                    'Longitude': -49.1371},\n",
      "                                                                                                   {'Latitude': 48.07948,\n",
      "                                                                                                    'Longitude': -50.50789}]}}]}}},\n",
      " 'TemporalExtent': {'RangeDateTime': {'BeginningDateTime': '2024-06-01T16:52:22Z',\n",
      "                                      'EndingDateTime': '2024-06-01T16:57:21Z'}}}\n",
      "\n",
      "Possible filename: PACE_OCI.20240601T165222.L2.OC_AOP.V3_0.nc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import earthaccess\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Authenticate\n",
    "auth = earthaccess.login(persist=True)\n",
    "\n",
    "# Parameters\n",
    "bbox = (-83.62, 41.34, -82, 42.27)      # (lon_min, lat_min, lon_max, lat_max)\n",
    "res = 0.01\n",
    "start_date = datetime(2024, 4, 14)\n",
    "end_date = datetime(2025, 5, 23)\n",
    "window_size = 7\n",
    "decay = 0.8\n",
    "\n",
    "# Ensure data directories exist\n",
    "os.makedirs(\"../Data/\", exist_ok=True)\n",
    "os.makedirs(\"../Cache/\", exist_ok=True)\n",
    "os.makedirs(\"../Images/\", exist_ok=True)\n",
    "\n",
    "# Output grid\n",
    "lat_bins = np.arange(bbox[1], bbox[3] + res, res)\n",
    "lon_bins = np.arange(bbox[0], bbox[2] + res, res)\n",
    "lat_centers = 0.5 * (lat_bins[:-1] + lat_bins[1:])\n",
    "lon_centers = 0.5 * (lon_bins[:-1] + lon_bins[1:])\n",
    "nlat, nlon = len(lat_centers), len(lon_centers)\n",
    "\n",
    "# --- Retry Helpers ---\n",
    "\n",
    "def safe_search(short_name, temporal, bounding_box, max_retries=500):\n",
    "    retries = 0\n",
    "    while True:\n",
    "        try:\n",
    "            results = earthaccess.search_data(\n",
    "                short_name=short_name,\n",
    "                temporal=temporal,\n",
    "                bounding_box=bounding_box\n",
    "            )\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            if retries >= max_retries:\n",
    "                print(f\"Search failed after {max_retries} retries: {e}\")\n",
    "                return []\n",
    "            wait = 5 + random.uniform(0, 3)\n",
    "            print(f\"Search error: {e}. Retrying in {wait:.1f} seconds...\")\n",
    "            time.sleep(wait)\n",
    "            \n",
    "results = safe_search(\n",
    "        short_name=\"PACE_OCI_L2_AOP\",\n",
    "        temporal=(\"2024-06-01\", \"2024-06-05\"),\n",
    "        bounding_box=bbox,\n",
    "    )\n",
    "if results:\n",
    "    print(\"\\nFirst granule keys:\")\n",
    "    print(results[0]['umm'].keys())\n",
    "    print(\"\\nFirst granule example:\")\n",
    "    import pprint\n",
    "    pprint.pprint(results[0]['umm'])\n",
    "    print(\"\\nPossible filename:\", )\n",
    "else:\n",
    "    print(\"No results found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d6969-f718-4f40-9af9-249dd0e77441",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave = xr.open_dataset(paths[0], group=\"sensor_band_parameters\")[\"wavelength_3d\"].data\n",
    "indices = np.where(wave == 450)\n",
    "indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc086c5-7122-45f3-81e2-2c782ed8caae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def generate_day_images(n, r_idx=113, g_idx=84, b_idx=42):\n",
    "    \"\"\"\n",
    "    Generate individual true-color images for the first n days in the composite data.\n",
    "    \n",
    "    Args:\n",
    "        n (int): Number of days to generate images for.\n",
    "        r_idx (int): Index of the wavelength to use for red channel.\n",
    "        g_idx (int): Index of the wavelength to use for green channel.\n",
    "        b_idx (int): Index of the wavelength to use for blue channel.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    data_path = \"../Images/composite_data.npy\"\n",
    "    meta_path = \"../Images/composite_metadata.pkl\"\n",
    "    if not os.path.exists(data_path) or not os.path.exists(meta_path):\n",
    "        print(\"Required files not found. Run the composite script first.\")\n",
    "        return\n",
    "    \n",
    "    data = np.load(data_path)\n",
    "    print(\"data.shape\", data.shape)\n",
    "    with open(meta_path, \"rb\") as f:\n",
    "        meta = pickle.load(f)\n",
    "    \n",
    "    lat = meta[\"lat\"]\n",
    "    lon = meta[\"lon\"]\n",
    "    wavelengths = meta[\"wavelengths\"]\n",
    "\n",
    "    num_days = data.shape[0]\n",
    "    n = min(n, num_days)  # Ensure n doesn't exceed available data\n",
    "\n",
    "    # Normalize Rrs data (assumed 0-0.03)\n",
    "    def normalize(arr, vmin=0, vmax=0.03):\n",
    "        return np.clip((arr - vmin) / (vmax - vmin), 0, 1)\n",
    "\n",
    "    for day_idx in range(n):\n",
    "        daily_data = data[day_idx]  # shape (h, w, c)\n",
    "\n",
    "        if np.isnan(daily_data).all():\n",
    "            print(f\"Day {day_idx + 1} has no valid data. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        r = normalize(daily_data[:, :, r_idx])\n",
    "        g = normalize(daily_data[:, :, g_idx])\n",
    "        b = normalize(daily_data[:, :, b_idx])\n",
    "\n",
    "        rgb = np.stack([r, g, b], axis=-1)\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(rgb, origin=\"lower\", extent=[lon.min(), lon.max(), lat.min(), lat.max()])\n",
    "        plt.title(f\"True-Color Image - Day {day_idx + 1}\")\n",
    "        plt.xlabel(\"Longitude\")\n",
    "        plt.ylabel(\"Latitude\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        out_path = f\"../Images/dayyy_{day_idx + 1:03d}.png\"\n",
    "        plt.savefig(out_path)\n",
    "        plt.close()\n",
    "        print(f\"Saved image: {out_path}\")\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625720e2-c137-45e6-88b7-f761342b9dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_day_images(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e649b557-ffe8-4ca9-b7ad-8baa86933821",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[3][45][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c903c7d4-b244-49c8-b3df-d82ea11672f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc5ce5c-1172-4cbb-8c47-25e12f570eb8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2d9896-4018-4edb-8308-3858765d57b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import earthaccess\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Authenticate\n",
    "auth = earthaccess.login(persist=True)\n",
    "\n",
    "# Parameters\n",
    "selected_wavelengths = [645, 555, 450]  # R, G, B\n",
    "bbox = (-83.62, 41.34, -82, 42.27)      # (lon_min, lat_min, lon_max, lat_max)\n",
    "res = 0.01  # grid resolution in degrees\n",
    "start_date = datetime(2024, 5, 17)\n",
    "end_date = datetime(2025, 5, 23)\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(\"../Images/\", exist_ok=True)\n",
    "os.makedirs(\"../Data/\", exist_ok=True)\n",
    "\n",
    "# Output grid\n",
    "lat_bins = np.arange(bbox[1], bbox[3] + res, res)\n",
    "lon_bins = np.arange(bbox[0], bbox[2] + res, res)\n",
    "lat_centers = 0.5 * (lat_bins[:-1] + lat_bins[1:])\n",
    "lon_centers = 0.5 * (lon_bins[:-1] + lon_bins[1:])\n",
    "nlat, nlon = len(lat_centers), len(lon_centers)\n",
    "\n",
    "# Iterate through each day\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    window_start = current_date - timedelta(days=4)\n",
    "    window_end = current_date\n",
    "    print(f\"Processing window: {window_start.date()} to {window_end.date()}\")\n",
    "\n",
    "    results = earthaccess.search_data(\n",
    "        short_name=\"PACE_OCI_L2_AOP\",\n",
    "        temporal=(window_start.strftime(\"%Y-%m-%d\"), window_end.strftime(\"%Y-%m-%d\")),\n",
    "        bounding_box=bbox,\n",
    "    )\n",
    "\n",
    "    if not results:\n",
    "        print(f\"No data for {window_end.strftime('%Y-%m-%d')}. Skipping.\")\n",
    "        current_date += timedelta(days=1)\n",
    "        continue\n",
    "\n",
    "    paths = earthaccess.download(results, \"../Data/\")\n",
    "\n",
    "    if not paths:\n",
    "        print(f\"No files downloaded for {window_end.strftime('%Y-%m-%d')}. Skipping.\")\n",
    "        current_date += timedelta(days=1)\n",
    "        continue\n",
    "\n",
    "    # Initialize sum and count arrays\n",
    "    sum_rgb = np.zeros((3, nlat, nlon))\n",
    "    count_rgb = np.zeros((3, nlat, nlon))\n",
    "\n",
    "    try:\n",
    "        wave = xr.open_dataset(paths[0], group=\"sensor_band_parameters\")[\"wavelength_3d\"].data\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read wavelength data: {e}\")\n",
    "        current_date += timedelta(days=1)\n",
    "        continue\n",
    "\n",
    "    # Process each file\n",
    "    for path in paths:\n",
    "        print(f\"Processing {path}\")\n",
    "        try:\n",
    "            rrs_ds = xr.open_dataset(path, group=\"geophysical_data\")[\"Rrs\"]\n",
    "            rrs_ds = rrs_ds.assign_coords(wavelength_3d=wave)\n",
    "\n",
    "            nav = xr.open_dataset(path, group=\"navigation_data\")\n",
    "            lat = nav[\"latitude\"].values\n",
    "            lon = nav[\"longitude\"].values\n",
    "\n",
    "            for b, wl in enumerate(selected_wavelengths):\n",
    "                band = rrs_ds.sel(wavelength_3d=wl, method=\"nearest\").values\n",
    "                mask = (\n",
    "                    np.isfinite(band) &\n",
    "                    (lat >= bbox[1]) & (lat <= bbox[3]) &\n",
    "                    (lon >= bbox[0]) & (lon <= bbox[2])\n",
    "                )\n",
    "\n",
    "                lat_valid = lat[mask]\n",
    "                lon_valid = lon[mask]\n",
    "                val_valid = band[mask]\n",
    "\n",
    "                lat_idx = np.searchsorted(lat_bins, lat_valid) - 1\n",
    "                lon_idx = np.searchsorted(lon_bins, lon_valid) - 1\n",
    "\n",
    "                for j in range(len(val_valid)):\n",
    "                    if 0 <= lat_idx[j] < nlat and 0 <= lon_idx[j] < nlon:\n",
    "                        sum_rgb[b, lat_idx[j], lon_idx[j]] += val_valid[j]\n",
    "                        count_rgb[b, lat_idx[j], lon_idx[j]] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {path}: {e}\")\n",
    "\n",
    "    # Compute mean reflectance\n",
    "    with np.errstate(invalid='ignore', divide='ignore'):\n",
    "        mean_rgb = sum_rgb / count_rgb\n",
    "        mean_rgb = np.nan_to_num(mean_rgb, nan=0.0)\n",
    "\n",
    "    # Normalize reflectance for display (Rrs units are ~0–0.03)\n",
    "    def normalize(arr, vmin=0, vmax=0.03):\n",
    "        return np.clip((arr - vmin) / (vmax - vmin), 0, 1)\n",
    "\n",
    "    r = normalize(mean_rgb[0])\n",
    "    g = normalize(mean_rgb[1])\n",
    "    b = normalize(mean_rgb[2])\n",
    "    rgb = np.stack([r, g, b], axis=-1)\n",
    "\n",
    "    # Save true color image\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(rgb, origin=\"lower\", extent=[bbox[0], bbox[2], bbox[1], bbox[3]])\n",
    "    plt.title(f\"5-Day Composite Ending {window_end.strftime('%Y-%m-%d')}\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.tight_layout()\n",
    "    out_path = f\"../Images/{window_end.strftime('%Y%m%d')}.png\"\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved image: {out_path}\")\n",
    "\n",
    "    # Clean up only the earliest date in window\n",
    "    delete_date = window_start.strftime('%Y%m%d')\n",
    "    for fname in os.listdir(\"../Data/\"):\n",
    "        if delete_date in fname and fname.endswith(\".nc\"):\n",
    "            try:\n",
    "                os.remove(os.path.join(\"../Data/\", fname))\n",
    "                print(f\"Deleted old file: {fname}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not delete {fname}: {e}\")\n",
    "\n",
    "    current_date += timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be17e289-34f7-495b-a54d-b72a044f0427",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
