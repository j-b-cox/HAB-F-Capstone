{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b47b11-20ac-417a-a5d2-2a2ce4ddb3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a2b04d5-073e-497b-9e65-757a7fa24b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_filename = './training_data_PACE.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39b92df7-68d3-4264-b00e-01b040028b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_patch(patch, shape=(5, 5, 172)):\n",
    "    \"\"\"Generate augmented versions of a (h, w, c) patch\"\"\"\n",
    "    patch = patch.reshape(shape)\n",
    "    aug_patches = []\n",
    "\n",
    "    # Original\n",
    "    aug_patches.append(patch)\n",
    "    \n",
    "    # Horizontal flip\n",
    "    aug_patches.append(np.flip(patch, axis=1))\n",
    "    \n",
    "    # Vertical flip\n",
    "    aug_patches.append(np.flip(patch, axis=0))\n",
    "    \n",
    "    # Rotations\n",
    "    aug_patches.append(np.rot90(patch, k=1, axes=(0, 1)))\n",
    "    aug_patches.append(np.rot90(patch, k=2, axes=(0, 1)))\n",
    "    aug_patches.append(np.rot90(patch, k=3, axes=(0, 1)))\n",
    "\n",
    "    return [p.flatten() for p in aug_patches]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efef6744-2bfc-4f34-b3b0-bc6ec9404e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded scaler and model.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# ─── Scale features & predict ─────────────────────────────────────────────────\u001b[39;00m\n\u001b[32m     80\u001b[39m features_scaled = scaler.transform(features)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m probs           = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_scaled\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[32m1\u001b[39m]\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# ─── Scatter predictions back into a 2D map ──────────────────────────────────\u001b[39;00m\n\u001b[32m     84\u001b[39m pred_map = np.full((ny, nx), np.nan)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1454\u001b[39m, in \u001b[36mLogisticRegression.predict_proba\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m   1449\u001b[39m ovr = \u001b[38;5;28mself\u001b[39m.multi_class \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33movr\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwarn\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1450\u001b[39m     \u001b[38;5;28mself\u001b[39m.multi_class \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdeprecated\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1451\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.classes_.size <= \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.solver == \u001b[33m\"\u001b[39m\u001b[33mliblinear\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1452\u001b[39m )\n\u001b[32m   1453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ovr:\n\u001b[32m-> \u001b[39m\u001b[32m1454\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_predict_proba_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1455\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1456\u001b[39m     decision = \u001b[38;5;28mself\u001b[39m.decision_function(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/sklearn/linear_model/_base.py:390\u001b[39m, in \u001b[36mLinearClassifierMixin._predict_proba_lr\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_predict_proba_lr\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Probability estimation for OvR logistic regression.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Positive class probabilities are computed as\u001b[39;00m\n\u001b[32m    387\u001b[39m \u001b[33;03m    1. / (1. + np.exp(-self.decision_function(X)));\u001b[39;00m\n\u001b[32m    388\u001b[39m \u001b[33;03m    multiclass is handled by normalizing that over all classes.\u001b[39;00m\n\u001b[32m    389\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     prob = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    391\u001b[39m     expit(prob, out=prob)\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m prob.ndim == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/sklearn/linear_model/_base.py:352\u001b[39m, in \u001b[36mLinearClassifierMixin.decision_function\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    349\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    350\u001b[39m xp, _ = get_namespace(X)\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    353\u001b[39m scores = safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m.coef_.T, dense_output=\u001b[38;5;28;01mTrue\u001b[39;00m) + \u001b[38;5;28mself\u001b[39m.intercept_\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    355\u001b[39m     xp.reshape(scores, (-\u001b[32m1\u001b[39m,))\n\u001b[32m    356\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (scores.ndim > \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m scores.shape[\u001b[32m1\u001b[39m] == \u001b[32m1\u001b[39m)\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m scores\n\u001b[32m    358\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/sklearn/utils/validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/sklearn/utils/validation.py:1105\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1099\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1114\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/sklearn/utils/validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/sklearn/utils/validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "from helpers import regrid_granule, extract_pace_patch, process_pace_granule\n",
    "\n",
    "# ─── User parameters ───────────────────────────────────────────────────────────\n",
    "MODEL_DIR     = \"models\"\n",
    "SCALER_FILE   = os.path.join(MODEL_DIR, \"scaler.pkl\")\n",
    "MODEL_FILE    = os.path.join(MODEL_DIR, \"logistic_regression.pkl\")\n",
    "\n",
    "GRANULE_FILE  = \"./data/PACE_OCI.20240603T180158.L2.OC_AOP.V3_0.nc\"\n",
    "\n",
    "BBOX          = (-83.5, 41.3, -82.45, 42.2)  # (lon_min, lat_min, lon_max, lat_max)\n",
    "RES_KM        = 1.2                          # target km resolution for regridding\n",
    "PATCH_SIZE    = 5\n",
    "COVERAGE_MIN  = 0.40                         # minimum fraction of non-NaN in patch\n",
    "\n",
    "params = {\n",
    "            \"short_names\": [\"PACE_OCI_L2_AOP\", \"PACE_OCI_L2_AOP_NRT\"],\n",
    "            \"start_date\": datetime(2024,2,1),\n",
    "            \"res_km\": 1.2,\n",
    "            \"pixel_count\": 5,\n",
    "        }\n",
    "params[\"sensor\"] = \"PACE\"\n",
    "params[\"bbox\"] = (-83.5, 41.3, -82.45, 42.2)\n",
    "wave_all = xr.open_dataset(GRANULE_FILE, group=\"sensor_band_parameters\")[\"wavelength_3d\"].data\n",
    "\n",
    "\n",
    "\n",
    "# ─── Load scaler & model ──────────────────────────────────────────────────────\n",
    "scaler = joblib.load(SCALER_FILE)\n",
    "model  = joblib.load(MODEL_FILE)\n",
    "print(\"Loaded scaler and model.\")\n",
    "\n",
    "wls, arr_stack, lat_centers, lon_centers = process_pace_granule(GRANULE_FILE, BBOX, params, wave_all)\n",
    "ny, nx = len(lat_centers), len(lon_centers)                     \n",
    "\n",
    "# ─── Extract patches & build feature matrix ───────────────────────────────────\n",
    "features = []\n",
    "coords   = []\n",
    "\n",
    "for i, lat0 in enumerate(lat_centers):\n",
    "    for j, lon0 in enumerate(lon_centers):\n",
    "        patch_dict = extract_pace_patch(\n",
    "            arr_stack=arr_stack,\n",
    "            wavelengths=wave_all,\n",
    "            lon0=lon0,\n",
    "            lat0=lat0,\n",
    "            pixel_count=PATCH_SIZE,\n",
    "            lat_centers=lat_centers,\n",
    "            lon_centers=lon_centers\n",
    "        )\n",
    "        \n",
    "        # convert the dict into a 3D array, bands-first\n",
    "        # ensure you use the exact same wavelengths list/order\n",
    "        \n",
    "        patch_arr = np.stack([patch_dict[wl] for wl in wave_all], axis=0)\n",
    "        \n",
    "        # now patch_arr is shape (n_bands, PATCH_SIZE, PATCH_SIZE)\n",
    "        # make it float and test for NaNs\n",
    "        patch_arr = patch_arr.astype(np.float64)\n",
    "        \n",
    "        # pass patch_arr into your feature vector\n",
    "        feat = patch_arr.transpose(1,2,0).ravel()\n",
    "        features.append(feat)\n",
    "        coords.append((i, j))\n",
    "\n",
    "if not features:\n",
    "    raise RuntimeError(\"No valid patches found in the scene.\")\n",
    "\n",
    "features = np.vstack(features)  # (n_valid, n_features)\n",
    "features = np.nan_to_num(features, nan=0.0)\n",
    "\n",
    "# ─── Scale features & predict ─────────────────────────────────────────────────\n",
    "features_scaled = scaler.transform(features)\n",
    "probs           = model.predict_proba(features_scaled)[:, 1]\n",
    "\n",
    "# ─── Scatter predictions back into a 2D map ──────────────────────────────────\n",
    "pred_map = np.full((ny, nx), np.nan)\n",
    "for (i, j), p in zip(coords, probs):\n",
    "    pred_map[i, j] = p\n",
    "\n",
    "# ─── Plot the bloom‑probability map ───────────────────────────────────────────\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax  = fig.add_subplot(1,1,1, projection=ccrs.PlateCarree())\n",
    "mesh = ax.pcolormesh(\n",
    "    lon_centers, lat_centers, pred_map,\n",
    "    shading=\"auto\", vmin=0, vmax=1\n",
    ")\n",
    "ax.coastlines(resolution=\"10m\")\n",
    "ax.set_extent([lon_min, lon_max, lat_min, lat_max])\n",
    "cbar = plt.colorbar(mesh, ax=ax, orientation=\"vertical\", label=\"Bloom Probability\")\n",
    "plt.title(\"Model‑Predicted Algal Bloom Probability\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d5fba1-5c14-4f61-940f-0eb01b4ab119",
   "metadata": {},
   "outputs": [],
   "source": [
    "84*73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926e6c83-41bc-43c1-8e05-b7373d7fe89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose `data` is your loaded array of entries, and you build X, y as before:\n",
    "# e.g., data = np.load(..., allow_pickle=True); \n",
    "# each entry: (granule_name, some_id, labels_tuple, patch_array)\n",
    "# Build lists:\n",
    "if not os.path.exists(training_data_filename):\n",
    "    raise FileNotFoundError(f\"File '{training_data_filename}' not found. Please ensure the correct path and rerun.\")\n",
    "\n",
    "# Load the training data\n",
    "data = np.load(training_data_filename, allow_pickle=True)\n",
    "\n",
    "print(f\"Loaded {len(data)} entries from {training_data_filename}\")\n",
    "\n",
    "\n",
    "\n",
    "# Determine feature vector size from first entry\n",
    "first_entry = data[0]\n",
    "if len(first_entry) < 3:\n",
    "    raise ValueError(\"Unexpected entry format. Each entry should be (row_index, labels_array, patch_flat_array)\")\n",
    "first_patch = first_entry[3]\n",
    "if not isinstance(first_patch, np.ndarray):\n",
    "    raise ValueError(\"Patch data not found or not a numpy array.\")\n",
    "\n",
    "\n",
    "# Build augmented X and y\n",
    "X_augmented = []\n",
    "y_augmented = []\n",
    "\n",
    "for entry in data:\n",
    "    _, _, labels, patch_flat = entry\n",
    "\n",
    "    patch_arr = np.nan_to_num(patch_flat, nan=0.0)\n",
    "\n",
    "    try:\n",
    "        \n",
    "        aug_patches = augment_patch(patch_arr, shape=(5,5,172))  # Adjust shape if needed\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping entry due to error in augmentation: {e}\")\n",
    "        continue\n",
    "\n",
    "    part = labels[4] if not np.isnan(labels[4]) else 0.0\n",
    "    part = part if part != 0 else 0.01\n",
    "    label = 1 if part >= 0.1 else 0\n",
    "\n",
    "    for aug_patch in aug_patches:\n",
    "        X_augmented.append(aug_patch)\n",
    "        y_augmented.append(label)\n",
    "\n",
    "X = np.array(X_augmented)\n",
    "y = np.array(y_augmented)\n",
    "\n",
    "print(f\"Augmented dataset: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "\n",
    "n_samples = len(data)\n",
    "indices = np.arange(n_samples)\n",
    "\n",
    "# Now split indices and labels; use indices to index X later\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"scikit-learn is required for this cell. Please install it (e.g., `pip install scikit-learn`).\") from e\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train logistic regression\n",
    "# For high-dimensional data, use solver='saga' or 'liblinear'; adjust max_iter as needed\n",
    "model = LogisticRegression(\n",
    "    solver='saga',\n",
    "    max_iter=10000,\n",
    "    class_weight = 'balanced',\n",
    "    random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Save scaler & model ---\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "scaler_path = os.path.join(\"models\", \"scaler.pkl\")\n",
    "model_path  = os.path.join(\"models\", \"logistic_regression.pkl\")\n",
    "\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Saved StandardScaler to {scaler_path}\")\n",
    "\n",
    "joblib.dump(model, model_path)\n",
    "print(f\"Saved LogisticRegression model to {model_path}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.colorbar()\n",
    "plt.xticks([0, 1])\n",
    "plt.yticks([0, 1])\n",
    "# Annotate counts\n",
    "thresh = cm.max() / 2\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "# Only if both classes present in test set\n",
    "if len(np.unique(y_test)) == 2:\n",
    "    y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, ls='--', alpha=0.5)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot plot ROC: only one class present in y_test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff41187-ef6e-4808-b6b2-f6644410fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b14d8-136b-49d2-a608-163d34b36a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8043fb8f-aee2-44ef-9f77-343c02dc9828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to import sklearn; if not installed, prompt user\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"scikit-learn is required for this cell. Please install it (e.g., `pip install scikit-learn`).\") from e\n",
    "\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train logistic regression\n",
    "# For high-dimensional data, use solver='saga' or 'liblinear'; adjust max_iter as needed\n",
    "model = LogisticRegression(\n",
    "    solver='saga',\n",
    "    max_iter=10000,\n",
    "    class_weight = 'balanced',\n",
    "    random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.colorbar()\n",
    "plt.xticks([0, 1])\n",
    "plt.yticks([0, 1])\n",
    "# Annotate counts\n",
    "thresh = cm.max() / 2\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "# Only if both classes present in test set\n",
    "if len(np.unique(y_test)) == 2:\n",
    "    y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, ls='--', alpha=0.5)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot plot ROC: only one class present in y_test.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93a7746-788b-419b-8b30-0117a8efa893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue scaling/training on X_train, y_train, etc.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='saga', max_iter=10000, random_state=42, class_weight='balanced')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Find misclassified indices (relative to the original data)\n",
    "mis_mask = (y_pred != y_test)\n",
    "mis_test_indices = test_idx[mis_mask]  # these are indices into the original data array\n",
    "\n",
    "# Recover granule names for misclassified samples\n",
    "mis_granule_names = [granule_names[i] for i in mis_test_indices]\n",
    "mis_station_names = [station_names[i] for i in mis_test_indices]\n",
    "\n",
    "print(f\"Number of misclassified samples in test set: {len(mis_granule_names)}\")\n",
    "for i in range(10):\n",
    "    print(\"Some stations:\", mis_granule_names[i], '-', mis_station_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159ebebf-9fa6-4bda-8333-53078bd7cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue scaling/training on X_train, y_train, etc.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='saga', max_iter=10000, random_state=42, class_weight='balanced')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "false_mask = (y_pred == 0)\n",
    "false_mask_indices = test_idx[false_mask]  # these are indices into the original data array\n",
    "\n",
    "true_mask = (y_pred == 1)\n",
    "true_mask_indices = test_idx[true_mask]  # these are indices into the original data array\n",
    "\n",
    "# Recover granule names for misclassified samples\n",
    "f_chla = [chls[i] for i in false_mask_indices]\n",
    "t_chla = [chls[i] for i in true_mask_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27d4624-ec15-4f6b-8eaa-af1a58a3865d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 64\n",
    "all_data = f_chla + t_chla  # combine to set common range\n",
    "range_min, range_max = min(all_data), max(all_data)\n",
    "\n",
    "# Plot\n",
    "plt.hist(f_chla, bins=bins, range=(range_min, 500), color='red', alpha=0.5, label='False')\n",
    "plt.hist(t_chla, bins=bins, range=(range_min, 500), color='blue', alpha=0.5, label='True')\n",
    "\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Two Arrays')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b436f2-a78d-4fb5-8fcc-1f95b69c7ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_chla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd4b7f5-f43f-456a-a622-dd7b94c2820d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
